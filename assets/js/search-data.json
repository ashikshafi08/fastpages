{
  
    
        "post0": {
            "title": "Title",
            "content": "For this experiment, we&#39;ll use a dataset from AI Crowd competition (live now) https://www.aicrowd.com/challenges/ai-blitz-8/problems/f1-team-classification . This is just for experiment purposes learning how to use tf.data.Dataset.from_generators() and this dataset was a suitable one to experiment with. . Creating a Dataset object from ImageDataGenerator . Since I am new to tensorflow and the tf.data api I wasn&#39;t sure how to construct complex pipelines. It was easy using ImageDataGenerator (high-level api) especially with directory and dataframe to load in images. . I came over this handy method tf.data.Dataset.from_generator() which help us createa a dataset object from the generator object itself. How cool? . Try to wrap the Dataset class around this data generators. . We will be looking into .flow_from_dataframe() method. . Things we&#39;ll be doing . Use transfer learning fine tuning to train our model | Use mixed_precision | Use prefetch | . !nvidia-smi . Thu May 20 20:35:10 2021 +--+ | NVIDIA-SMI 465.19.01 Driver Version: 460.32.03 CUDA Version: 11.2 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 Off | 00000000:00:04.0 Off | 0 | | N/A 61C P8 11W / 70W | 0MiB / 15109MiB | 0% Default | | | | N/A | +-+-+-+ +--+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +--+ . !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py . --2021-05-20 20:35:11-- https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 10246 (10K) [text/plain] Saving to: ‘helper_functions.py’ helper_functions.py 100%[===================&gt;] 10.01K --.-KB/s in 0s 2021-05-20 20:35:11 (59.5 MB/s) - ‘helper_functions.py’ saved [10246/10246] . from helper_functions import plot_loss_curves , compare_historys . !pip install aicrowd-cli API_KEY = &#39;#########&#39; !aicrowd login --api-key $API_KEY . !aicrowd dataset download --challenge f1-team-classification -j 3 !rm -rf data !mkdir data !unzip train.zip -d data/train !unzip val.zip -d data/val !unzip test.zip -d data/test !mv train.csv data/train.csv !mv val.csv data/val.csv !mv sample_submission.csv data/sample_submission.csv . train_dir = &#39;data/train/&#39; test_dir = &#39;data/test/&#39; val_dir = &#39;data/val/&#39; # Our ImageID and label dataframes import pandas as pd import numpy as np df_train = pd.read_csv(&#39;data/train.csv&#39;) df_val = pd.read_csv(&#39;data/val.csv&#39;) . df_train.head() . ImageID label . 0 0 | redbull | . 1 1 | mercedes | . 2 2 | redbull | . 3 3 | redbull | . 4 4 | redbull | . Becoming one with the data . Alright now we&#39;ve got our data and it&#39;s time to visualize it and see how they look. . df_train[&#39;label&#39;].value_counts() . redbull 20043 mercedes 19957 Name: label, dtype: int64 . df_train[&#39;ImageID&#39;].shape . (40000,) . import tensorflow as tf BATCH_SIZE = 64 IMG_SIZE = (224 , 224) . train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255.) valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255.) . import os print(os.listdir(train_dir)[:10]) . [&#39;33821.jpg&#39;, &#39;30578.jpg&#39;, &#39;27097.jpg&#39;, &#39;25608.jpg&#39;, &#39;5723.jpg&#39;, &#39;18465.jpg&#39;, &#39;15993.jpg&#39;, &#39;16175.jpg&#39;, &#39;5418.jpg&#39;, &#39;19479.jpg&#39;] . def append_ext(fn): return f&#39;{fn}.jpg&#39; # Now applying our function df_train[&#39;ImageID&#39;] = df_train[&#39;ImageID&#39;].apply(append_ext) df_val[&#39;ImageID&#39;] = df_val[&#39;ImageID&#39;].apply(append_ext) . df_train[&#39;ImageID&#39;][:5] . 0 0.jpg 1 1.jpg 2 2.jpg 3 3.jpg 4 4.jpg Name: ImageID, dtype: object . train_data_all = train_datagen.flow_from_dataframe(dataframe= df_train , directory = train_dir , x_col = &#39;ImageID&#39; , y_col = &#39;label&#39; , target_size = IMG_SIZE , class_mode = &#39;binary&#39; , batch_size = 32 , shuffle = True) val_data_all = valid_datagen.flow_from_dataframe(dataframe = df_val , directory = val_dir , x_col = &#39;ImageID&#39; , y_col = &#39;label&#39; , target_size = IMG_SIZE , class_mode = &#39;binary&#39;, batch_size = 32 , shuffle = True) # Without any transformations (batch_size , imgsize etc..) train_data_none = train_datagen.flow_from_dataframe(dataframe= df_train , directory = train_dir , x_col = &#39;ImageID&#39; , y_col = &#39;label&#39; , batch_size = 32 , class_mode = &#39;binary&#39; ) val_data_none = valid_datagen.flow_from_dataframe(dataframe = df_val , directory = val_dir , x_col = &#39;ImageID&#39; , y_col = &#39;label&#39; , batch_size = 32, class_mode = &#39;binary&#39;) . Found 40000 validated image filenames belonging to 2 classes. Found 4000 validated image filenames belonging to 2 classes. Found 40000 validated image filenames belonging to 2 classes. Found 4000 validated image filenames belonging to 2 classes. . images, labels = next(train_data_all) # Checking their shapes and dtypes images.shape , labels.shape , images.dtype , labels.dtype . ((32, 224, 224, 3), (32,), dtype(&#39;float32&#39;), dtype(&#39;float32&#39;)) . images_none , labels_none = next(train_data_none) # Checking their shapes and dtypes images_none.shape , labels_none.shape , images_none.dtype , labels_none.dtype . ((32, 256, 256, 3), (32,), dtype(&#39;float32&#39;), dtype(&#39;float32&#39;)) . train_data_all.class_indices . {&#39;mercedes&#39;: 0, &#39;redbull&#39;: 1} . Creating a dataset using tf.data.Dataset.from_generators() . Now we&#39;re going to convert the generator into Dataset object using the tf.data.Dataset.from_generator() . Things to be noted: . In the place of lambda use your datagenerator object. | The output_shapes is really important because our dataset object will returns the exact shape we mention inside the output_shapes. | . This was the reason we examined our data types and shape above as soon as we built our generator. . Creating a dataset with the transforms here (just for experimentation) . train_dataset_all = tf.data.Dataset.from_generator( lambda: train_data_all , output_types = (tf.float32 , tf.float32) , output_shapes = ([32 , 224 , 224 , 3] , [32 , ]) ) valid_dataset_all = tf.data.Dataset.from_generator( lambda: val_data_all , output_types = (tf.float32 , tf.float32), output_shapes = ([32 , 224 , 224 , 3] , [32 , ]) ) train_dataset_all , valid_dataset_all . (&lt;FlatMapDataset shapes: ((32, 224, 224, 3), (32,)), types: (tf.float32, tf.float32)&gt;, &lt;FlatMapDataset shapes: ((32, 224, 224, 3), (32,)), types: (tf.float32, tf.float32)&gt;) . Creating a dataset without any transforms (just for experimentations) . train_dataset_none = tf.data.Dataset.from_generator( lambda: train_data_none , output_types = (tf.float32 , tf.float32) , output_shapes = ([32 , 256 , 256 , 3] , [32 , ]) ) valid_dataset_none = tf.data.Dataset.from_generator( lambda: val_data_all , output_types = (tf.float32 , tf.float32), output_shapes = ([32 , 256 , 256 , 3] , [32 , ]) ) train_dataset_none , valid_dataset_none . (&lt;FlatMapDataset shapes: ((32, 256, 256, 3), (32,)), types: (tf.float32, tf.float32)&gt;, &lt;FlatMapDataset shapes: ((32, 256, 256, 3), (32,)), types: (tf.float32, tf.float32)&gt;) . Note . Since we&#39;re derived our from dataset object from a generator we won&#39;t be able to use len() function to know the number of samples in our dataset. . We can use cardinality to get the number of samples in our dataset. It&#39;s because in our case after the conversion the length is unknown and infinite. . tf.data.experimental.cardinality --&gt; returns cardinality of the dataset . This will return -2 for now. . It should return 40000 (for train) because that was the number of samples (images) in our train directory. . But don&#39;t worry we can even fix this by using a similar function, since our length is unknown and it&#39;s the common case when you convert from generator to a dataset object. . We can explicitly enter our number of samples and even better we can use the len() function now on our dataset using, . tf.data.experimental.assert_cardinality() --&gt; Asserts the cardinality of the dataset. Now will apply this to our dataset. . train_dataset_all = train_dataset_all.apply(tf.data.experimental.assert_cardinality(40000)) valid_dataset_all = valid_dataset_all.apply(tf.data.experimental.assert_cardinality(4000)) # Same for our without transformations dataset train_dataset_none = train_dataset_none.apply(tf.data.experimental.assert_cardinality(40000)) valid_dataset_none = valid_dataset_none.apply(tf.data.experimental.assert_cardinality(4000)) . train_dataset_all , valid_dataset_all . (&lt;_AssertCardinalityDataset shapes: ((32, 224, 224, 3), (32,)), types: (tf.float32, tf.float32)&gt;, &lt;_AssertCardinalityDataset shapes: ((32, 224, 224, 3), (32,)), types: (tf.float32, tf.float32)&gt;) . len(train_dataset_all) , len(valid_dataset_all) . (40000, 4000) . from tensorflow.keras import mixed_precision mixed_precision.set_global_policy(policy = &#39;mixed_float16&#39;) . INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: Tesla T4, compute capability 7.5 . mixed_precision.global_policy() # should output &quot;mixed_float16&quot; . &lt;Policy &#34;mixed_float16&#34;&gt; . train_data_all.class_indices . {&#39;mercedes&#39;: 0, &#39;redbull&#39;: 1} . import matplotlib.pyplot as plt x , y = train_data.next() for i in range(0, 4): image = x[i] label = y[i] plt.axis(False) # print(label) --&gt; for checking whether it&#39;s plotting right ones if label == 1.0: label = &#39;redbull&#39; else: label = &#39;mercedes&#39; plt.title(label) plt.imshow(image) plt.show() . class_names = list(train_data_all.class_indices.keys()) len(class_names) . 2 . Modelling . from tensorflow.keras import layers from tensorflow.keras.layers.experimental import preprocessing # Create base model input_shape = (224, 224, 3) base_model = tf.keras.applications.EfficientNetB0(include_top=False) base_model.trainable = False # freeze base model layers # Create Functional model inputs = layers.Input(shape=input_shape, name=&quot;input_layer&quot;) # Note: EfficientNetBX models have rescaling built-in but if your model didn&#39;t you could have a layer like below # x = preprocessing.Rescaling(1./255)(x) x = base_model(inputs, training=False) # set base_model to inference mode only x = layers.GlobalAveragePooling2D(name=&quot;pooling_layer&quot;)(x) x = layers.Dense(1)(x) # want one output neuron per class # Separate activation of output layer so we can output float32 activations outputs = layers.Activation(&quot;sigmoid&quot;, dtype=tf.float32, name=&quot;softmax_float32&quot;)(x) model_1 = tf.keras.Model(inputs, outputs) . for layer in model_1.layers: print(layer.name , layer.trainable , layer.dtype , layer.dtype_policy) . input_layer True float32 &lt;Policy &#34;float32&#34;&gt; efficientnetb0 False float32 &lt;Policy &#34;mixed_float16&#34;&gt; pooling_layer True float32 &lt;Policy &#34;mixed_float16&#34;&gt; dense_5 True float32 &lt;Policy &#34;mixed_float16&#34;&gt; softmax_float32 True float32 &lt;Policy &#34;float32&#34;&gt; . !pip install tensorflow_addons import tensorflow_addons as tfa f1_score = tfa.metrics.F1Score(average=&#39;macro&#39; , num_classes= 1) . Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.13.0) Requirement already satisfied: typeguard&gt;=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1) . model_1.compile(loss = tf.keras.losses.BinaryCrossentropy() , optimizer = tf.keras.optimizers.Adam() , metrics = [&#39;accuracy&#39; , f1_score]) . Let&#39;s train the model again . Note:Before using len(train_data) in the steps_per_epoch we should divide it by our batch_size. . len(train_dataset_all) // 64 . 625 . history_feature_model_1 = model_1.fit(train_dataset_all , steps_per_epoch = len(train_dataset_all) // 32, epochs = 3 , validation_data = valid_dataset_all, validation_steps = int(0.15 * (len(valid_dataset_all))) ) . Epoch 1/3 1250/1250 [==============================] - 152s 118ms/step - loss: 0.6947 - accuracy: 0.4989 - f1_score: 0.6667 - val_loss: 0.6931 - val_accuracy: 0.5055 - val_f1_score: 0.6618 Epoch 2/3 1250/1250 [==============================] - 146s 116ms/step - loss: 0.6947 - accuracy: 0.5034 - f1_score: 0.6633 - val_loss: 0.6983 - val_accuracy: 0.5049 - val_f1_score: 0.6623 Epoch 3/3 1250/1250 [==============================] - 145s 116ms/step - loss: 0.6948 - accuracy: 0.4920 - f1_score: 0.6639 - val_loss: 0.6957 - val_accuracy: 0.4948 - val_f1_score: 0.6621 . base_model.trainable = True # Refreeze all layers except last 5 for layer in base_model.layers[:-3]: layer.trainable = False # Compiling the model again making the change model_1.compile(loss = tf.keras.losses.BinaryCrossentropy() , optimizer = tf.keras.optimizers.Adam(lr = 0.0001) , metrics = [&#39;accuracy&#39; , f1_score]) . reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=&quot;val_loss&quot;, factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x) patience=2, verbose=1, # print out when learning rate goes down min_lr=1e-7) . initial_epochs = 5 fine_tune_epochs = initial_epochs + 25 history_fine_model_1 = model_1.fit(train_dataset_all , steps_per_epoch = len(train_dataset_all) // 32 , epochs = fine_tune_epochs , initial_epoch = history_feature_model_1.epoch[-1] , validation_data = valid_dataset_all , validation_steps = int(0.15 * (len(valid_dataset_all))) , callbacks = [reduce_lr]) . Epoch 3/30 1250/1250 [==============================] - 151s 118ms/step - loss: 0.6951 - accuracy: 0.5050 - f1_score: 0.6656 - val_loss: 0.6951 - val_accuracy: 0.4953 - val_f1_score: 0.6624 Epoch 4/30 1250/1250 [==============================] - 145s 116ms/step - loss: 0.6944 - accuracy: 0.5048 - f1_score: 0.6677 - val_loss: 0.6932 - val_accuracy: 0.5073 - val_f1_score: 0.6602 Epoch 5/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6947 - accuracy: 0.4983 - f1_score: 0.6681 - val_loss: 0.6939 - val_accuracy: 0.4971 - val_f1_score: 0.6641 Epoch 6/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6943 - accuracy: 0.5001 - f1_score: 0.6683 - val_loss: 0.6930 - val_accuracy: 0.5061 - val_f1_score: 0.6612 Epoch 7/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6941 - accuracy: 0.5010 - f1_score: 0.6701 - val_loss: 0.6933 - val_accuracy: 0.4938 - val_f1_score: 0.6611 Epoch 8/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6941 - accuracy: 0.4985 - f1_score: 0.6630 - val_loss: 0.6931 - val_accuracy: 0.5199 - val_f1_score: 0.6628 Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05. Epoch 9/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6933 - accuracy: 0.4955 - f1_score: 0.6656 - val_loss: 0.6931 - val_accuracy: 0.5056 - val_f1_score: 0.6616 Epoch 10/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6932 - accuracy: 0.5005 - f1_score: 0.6648 - val_loss: 0.6932 - val_accuracy: 0.4961 - val_f1_score: 0.6632 Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06. Epoch 11/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5013 - f1_score: 0.6666 - val_loss: 0.6932 - val_accuracy: 0.4939 - val_f1_score: 0.6612 Epoch 12/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5037 - f1_score: 0.6677 - val_loss: 0.6930 - val_accuracy: 0.5054 - val_f1_score: 0.6619 Epoch 00012: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07. Epoch 13/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6932 - accuracy: 0.4973 - f1_score: 0.6691 - val_loss: 0.6930 - val_accuracy: 0.5052 - val_f1_score: 0.6620 Epoch 14/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6932 - accuracy: 0.4954 - f1_score: 0.6728 - val_loss: 0.6931 - val_accuracy: 0.5091 - val_f1_score: 0.6610 Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.600000018697756e-07. Epoch 15/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5219 - f1_score: 0.6698 - val_loss: 0.6931 - val_accuracy: 0.4970 - val_f1_score: 0.6626 Epoch 16/30 1250/1250 [==============================] - 145s 116ms/step - loss: 0.6931 - accuracy: 0.5025 - f1_score: 0.6658 - val_loss: 0.6931 - val_accuracy: 0.5027 - val_f1_score: 0.6623 Epoch 00016: ReduceLROnPlateau reducing learning rate to 1e-07. Epoch 17/30 1250/1250 [==============================] - 145s 116ms/step - loss: 0.6931 - accuracy: 0.5117 - f1_score: 0.6680 - val_loss: 0.6931 - val_accuracy: 0.4972 - val_f1_score: 0.6609 Epoch 18/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5091 - f1_score: 0.6684 - val_loss: 0.6931 - val_accuracy: 0.4991 - val_f1_score: 0.6626 Epoch 00018: ReduceLROnPlateau reducing learning rate to 1e-07. Epoch 19/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5043 - f1_score: 0.6677 - val_loss: 0.6931 - val_accuracy: 0.4999 - val_f1_score: 0.6629 Epoch 20/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5048 - f1_score: 0.6670 - val_loss: 0.6931 - val_accuracy: 0.4975 - val_f1_score: 0.6609 Epoch 00020: ReduceLROnPlateau reducing learning rate to 1e-07. Epoch 21/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5094 - f1_score: 0.6707 - val_loss: 0.6931 - val_accuracy: 0.4972 - val_f1_score: 0.6626 Epoch 22/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5023 - f1_score: 0.6662 - val_loss: 0.6931 - val_accuracy: 0.5002 - val_f1_score: 0.6630 Epoch 00022: ReduceLROnPlateau reducing learning rate to 1e-07. Epoch 23/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5080 - f1_score: 0.6700 - val_loss: 0.6931 - val_accuracy: 0.4959 - val_f1_score: 0.6616 Epoch 24/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5017 - f1_score: 0.6656 - val_loss: 0.6931 - val_accuracy: 0.4978 - val_f1_score: 0.6614 Epoch 00024: ReduceLROnPlateau reducing learning rate to 1e-07. Epoch 25/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5032 - f1_score: 0.6659 - val_loss: 0.6931 - val_accuracy: 0.4993 - val_f1_score: 0.6627 Epoch 26/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5052 - f1_score: 0.6682 - val_loss: 0.6931 - val_accuracy: 0.4979 - val_f1_score: 0.6613 Epoch 00026: ReduceLROnPlateau reducing learning rate to 1e-07. Epoch 27/30 1250/1250 [==============================] - 147s 117ms/step - loss: 0.6931 - accuracy: 0.5065 - f1_score: 0.6702 - val_loss: 0.6931 - val_accuracy: 0.4976 - val_f1_score: 0.6628 Epoch 28/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5035 - f1_score: 0.6687 - val_loss: 0.6931 - val_accuracy: 0.4983 - val_f1_score: 0.6608 Epoch 00028: ReduceLROnPlateau reducing learning rate to 1e-07. Epoch 29/30 1250/1250 [==============================] - 147s 118ms/step - loss: 0.6931 - accuracy: 0.5078 - f1_score: 0.6692 - val_loss: 0.6931 - val_accuracy: 0.4980 - val_f1_score: 0.6615 Epoch 30/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5074 - f1_score: 0.6713 - val_loss: 0.6931 - val_accuracy: 0.5005 - val_f1_score: 0.6632 Epoch 00030: ReduceLROnPlateau reducing learning rate to 1e-07. . Log (should be improved) . Epoch 3/30 1250/1250 [==============================] - 151s 118ms/step - loss: 0.6951 - accuracy: 0.5050 - f1_score: 0.6656 - val_loss: 0.6951 - val_accuracy: 0.4953 - val_f1_score: 0.6624 Epoch 4/30 1250/1250 [==============================] - 145s 116ms/step - loss: 0.6944 - accuracy: 0.5048 - f1_score: 0.6677 - val_loss: 0.6932 - val_accuracy: 0.5073 - val_f1_score: 0.6602 Epoch 5/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6947 - accuracy: 0.4983 - f1_score: 0.6681 - val_loss: 0.6939 - val_accuracy: 0.4971 - val_f1_score: 0.6641 Epoch 6/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6943 - accuracy: 0.5001 - f1_score: 0.6683 - val_loss: 0.6930 - val_accuracy: 0.5061 - val_f1_score: 0.6612 Epoch 7/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6941 - accuracy: 0.5010 - f1_score: 0.6701 - val_loss: 0.6933 - val_accuracy: 0.4938 - val_f1_score: 0.6611 Epoch 8/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6941 - accuracy: 0.4985 - f1_score: 0.6630 - val_loss: 0.6931 - val_accuracy: 0.5199 - val_f1_score: 0.6628 . Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05. Epoch 9/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6933 - accuracy: 0.4955 - f1_score: 0.6656 - val_loss: 0.6931 - val_accuracy: 0.5056 - val_f1_score: 0.6616 Epoch 10/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6932 - accuracy: 0.5005 - f1_score: 0.6648 - val_loss: 0.6932 - val_accuracy: 0.4961 - val_f1_score: 0.6632 . Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06. Epoch 11/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5013 - f1_score: 0.6666 - val_loss: 0.6932 - val_accuracy: 0.4939 - val_f1_score: 0.6612 Epoch 12/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5037 - f1_score: 0.6677 - val_loss: 0.6930 - val_accuracy: 0.5054 - val_f1_score: 0.6619 . Epoch 00012: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07. Epoch 13/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6932 - accuracy: 0.4973 - f1_score: 0.6691 - val_loss: 0.6930 - val_accuracy: 0.5052 - val_f1_score: 0.6620 Epoch 14/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6932 - accuracy: 0.4954 - f1_score: 0.6728 - val_loss: 0.6931 - val_accuracy: 0.5091 - val_f1_score: 0.6610 . Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.600000018697756e-07. Epoch 15/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5219 - f1_score: 0.6698 - val_loss: 0.6931 - val_accuracy: 0.4970 - val_f1_score: 0.6626 Epoch 16/30 1250/1250 [==============================] - 145s 116ms/step - loss: 0.6931 - accuracy: 0.5025 - f1_score: 0.6658 - val_loss: 0.6931 - val_accuracy: 0.5027 - val_f1_score: 0.6623 . Epoch 00016: ReduceLROnPlateau reducing learning rate to 1e-07. Epoch 17/30 1250/1250 [==============================] - 145s 116ms/step - loss: 0.6931 - accuracy: 0.5117 - f1_score: 0.6680 - val_loss: 0.6931 - val_accuracy: 0.4972 - val_f1_score: 0.6609 Epoch 18/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5091 - f1_score: 0.6684 - val_loss: 0.6931 - val_accuracy: 0.4991 - val_f1_score: 0.6626 . Epoch 00018: ReduceLROnPlateau reducing learning rate to 1e-07. Epoch 19/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5043 - f1_score: 0.6677 - val_loss: 0.6931 - val_accuracy: 0.4999 - val_f1_score: 0.6629 Epoch 20/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5048 - f1_score: 0.6670 - val_loss: 0.6931 - val_accuracy: 0.4975 - val_f1_score: 0.6609 . Epoch 00020: ReduceLROnPlateau reducing learning rate to 1e-07. Epoch 21/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5094 - f1_score: 0.6707 - val_loss: 0.6931 - val_accuracy: 0.4972 - val_f1_score: 0.6626 Epoch 22/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5023 - f1_score: 0.6662 - val_loss: 0.6931 - val_accuracy: 0.5002 - val_f1_score: 0.6630 . Epoch 00022: ReduceLROnPlateau reducing learning rate to 1e-07. Epoch 23/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5080 - f1_score: 0.6700 - val_loss: 0.6931 - val_accuracy: 0.4959 - val_f1_score: 0.6616 Epoch 24/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5017 - f1_score: 0.6656 - val_loss: 0.6931 - val_accuracy: 0.4978 - val_f1_score: 0.6614 . Epoch 00024: ReduceLROnPlateau reducing learning rate to 1e-07. Epoch 25/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5032 - f1_score: 0.6659 - val_loss: 0.6931 - val_accuracy: 0.4993 - val_f1_score: 0.6627 Epoch 26/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5052 - f1_score: 0.6682 - val_loss: 0.6931 - val_accuracy: 0.4979 - val_f1_score: 0.6613 . Epoch 00026: ReduceLROnPlateau reducing learning rate to 1e-07. Epoch 27/30 1250/1250 [==============================] - 147s 117ms/step - loss: 0.6931 - accuracy: 0.5065 - f1_score: 0.6702 - val_loss: 0.6931 - val_accuracy: 0.4976 - val_f1_score: 0.6628 Epoch 28/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5035 - f1_score: 0.6687 - val_loss: 0.6931 - val_accuracy: 0.4983 - val_f1_score: 0.6608 . Epoch 00028: ReduceLROnPlateau reducing learning rate to 1e-07. Epoch 29/30 1250/1250 [==============================] - 147s 118ms/step - loss: 0.6931 - accuracy: 0.5078 - f1_score: 0.6692 - val_loss: 0.6931 - val_accuracy: 0.4980 - val_f1_score: 0.6615 Epoch 30/30 1250/1250 [==============================] - 146s 117ms/step - loss: 0.6931 - accuracy: 0.5074 - f1_score: 0.6713 - val_loss: 0.6931 - val_accuracy: 0.5005 - val_f1_score: 0.6632 . Epoch 00030: ReduceLROnPlateau reducing learning rate to 1e-07. . d .",
            "url": "https://ashikshafi08.github.io/fastpages/2021/05/23/Generator-to-Dataset.ipynb.html",
            "relUrl": "/2021/05/23/Generator-to-Dataset.ipynb.html",
            "date": " • May 23, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "How would I learn to code, if it's for now - Part 2",
            "content": "How would I learn to code, if it’s for now - Part 2 . . Thanks for landing on Part 2 of the series, still if you haven’t read (Part 1)[] check them out to understand what we are talking about. . In the earlier blog, we discussed the mindset and it was more about how you should prepare yourselves to tackle it throughout the journey. Getting the right mindset is really important, you can learn to code anywhere but bringing out the mindset all matters. . Mixing Creation and Consumption . For instance, let’s take you to want to learn to make a website. Before taking a jump in, you gotta make sure what are the things I gotta learn. Planning and knowing the topics you are going to learn will accelerate your learning journey. . If not, there are courses out there that do this for you. Go through their contents and check out the reviews about the course, if you are satisfied then go ahead. And for someone who couldn’t afford courses, you have to make your own research, I will leave some links below later so it can lower the hustle for you. . Alright, now we are all set, got the curriculum and a plan of what we are going to learn. The first step would be falling into the Consuming stage, where you will focus on learning new things that you haven’t come across. . This is where most people goof up, they fall into the tutorial hell without their own acknowledgment. We have a fix for this, and this is why mixing up creation and consumption comes in handy, not only you could just escape the tutorial hell but it also improve your learning. . The Plan . Rather than getting carried away with the course, have a 30 days rule. In this rule, you dedicate 20 days for consumption and 10 days for creation. . Spend your next 20 days on focusing just the course, don’t fall for anything. Your absolute focus should be on learning the material or course for the next 20 days. More preferably you can even choose the topics you would be learning for the next 20 days and focus on them. . I think 20 days is legit, it’s nominal and things come under in a one-month bed. . Next is the creation stage, remember after spending 20 days consuming a large number of materials now it’s time to put out them in a real test. With whatever you learned so far, put them to the test. There are a lot of sites out there that hold practice problems for you to work on, but it’s not the case you always get them right. . Chances are high you will fail to solve certain problems, but remember to try as much as you can in the span of 10 days. You are in the Creation stage, where your intent is just to create and practice, no matter things don’t get right but the idea here is to break the tutorial hell. . You can practice or solve problems sites like, . Hackerrank | TopCoder | FreeCodeCamp ( you can learn even here) | Edabit | Codechef | . And there is a lot out there, but out of my perspective, these are more than enough. If you cant find a solution for something within those 10 days, learn from others. There will be solutions posted up there by other programmers like you, if you cant reproduce the solution feel free to learn from other’s code. But don’t copy them!! . If you are learning web or app development, with the things you learned for 20 days trying to create something small of your own be it a landing page, just a static app whatever sparks your interest. . Do this 20/10 days rule over and over throughout your learning, take your time to observe, and take a pause but don’t cut the flow. By falling into this repetition chances are high you grasp the knowledge way better, courses out there don’t hold everything you need to know. . But spending 10 days on creating not only helps you quit tutorial hell but also you get to learn new things out of your course. . Fixing our forgetting curve . Even after spending hours and hours learning something we tend to forget them, especially the hard part at times. Our goal is to learn to program efficiently, you could ramp up the tutorials but the idea of recalling helps you to retain your learning curve. . . Above is the Ebbinghaus forgetting curve, if you observe the graph as the days increase your memory retention in % decreases. You remember things the best for a day or two, followed by you tend to have a drop in the memory as the days go by. . To learn more about Ebbinghaus forgetting curve check here. . You could have sensed the idea behind the forgetting curve, but we can fix this with something called spaced repetition. . . Spaced repetition is nothing but recalling what you learned, we could fix our forgetting curve by recalling things that we learned on daily basis. To do this, we could grab a helping hand app called (Anki)[https://medium.com/r/?url=https%3A%2F%2Fapps.ankiweb.net%2F], they are powerful flashcards that help you in the process of recalling things. . . You can read here how to get most out of Anki. . Above is an example of how you could Anki for your learning. You can use these flashcards to remember what does a particular syntax does, the meaning of that syntax, etc… . If you want to know more about how to use Anki for programming check out this video by Daniel Bourke. . Alright, let us wrap up with the pep talk thing for now. I Will dive into some resources I would go back now, to learn everything from scratch. I give out both paid and non-paid resources, the choice is up to you guys. . CS50: Introduction to Computer Science: You will encounter a Passionate teacher David J. Malan, the whole Computer Science topics even might make sense to a no-brainer. I prefer, starting your journey with this course and care about the rest of things later. This gives you a greater intuition and kicks start to the field of Computer Science. | Even they do have other courses related to different fields of Computer Science, check them out here on their Youtube Channel. . Computer Science Degree: Courses which can teach you computer science from scratch, if you couldn’t spend for a college degree, feel free to go through these courses which are better than most courses on universities. Teach yourself Computer Science (Free) | Open Source Society University (Free) | A Self-Learning, Modern Computer Science (Free) | . | FreeCodeCamp: My favorite resource, a great community with passionate individuals who are willing to offer free education. You will find a wide variety of resources even on their Youtube channel from game development to deep learning. | FreeCodeCamp’s Youtube Channel (Free) . Udemy: A well-known platform for online education, quality courses with great instructors teaching more than 35 million students are learning by this platform, with 57k teachers with great content. | Lemme list down certain courses which I had been through and I think they can benefit you too. . Complete Machine Learning &amp; Data Science Bootcamp 2021: This is a course suggested if you are interested in the field of Data Science. The good thing about this course is it will help you to get started, from there you could find your way out. Unlike most courses, cramping you with too much jargon, Daniel had done an intelligent job of teaching with great analogies which makes things easier. | 100 Days of Code - The Complete Python Pro Bootcamp 2021: Awesome course for people’s doesnt just like videos and wanna add a bit of challenge to it. As we discussed in the blog, prevention of falling into the tutorial hell. This course would do that and puts you on challenges every day and builds consistency. | The Web Developer Bootcamp 2021: Colt has redone the entire course and structured the Bootcamp for absolute beginners who just want to jump into the field of Web. You will get to learn great tools and frameworks throughout the course. | The Complete JavaScript Course 2020: From Zero to Expert! : If you are looking to learn Javascript, this is the course to do so. A comprehensive course that has advanced javascript topics like ES6+ and much more. 50+ Challenges and assignments to test your skills. | . The above resources are used by me, rather than throwing out fancy shits at you guys. If you cant afford courses, there are always free courses out there with better interpretation than the paid ones. . Get back to your learning only when you got a purpose. Learning becomes easy when you are learning something to solve a problem. Find the problem and learn the skills you need to solve them. . The two blogs are out of my experience and cutting out the jargon, giving you the methodology and the right mindset was the key behind this whole blog so far. . Let’s put an end to the whole thing for now. Thanks for tuning in! Good luck with your Journey. . Keep Learning! . Keep Creating! .",
            "url": "https://ashikshafi08.github.io/fastpages/programming/2021/04/24/hwltc_2.html",
            "relUrl": "/programming/2021/04/24/hwltc_2.html",
            "date": " • Apr 24, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "How would I learn to code, if it's for now - Part 1",
            "content": "How would I learn to code, if it’s for now - Part 1 . . What comes to your mind when you hear the word Programming? . Please, don’t ever say it’s HTML (or) CSS neither both isn’t a programming language. But still, we would have at least came across things like, . Java | Python | Javascript and the list go on… | . Before jumping into the blog, I am gonna cut out technical jargon as usual. This is not a blog from a technical perspective rather, one that would help you fix the learning better. . If you started to learn a Programming language, or even after learning you somewhere felt lost then this could be the blog might fix you . Why I am writing this? . You could ask me what made me write this? Well, simple, I am a self-taught Machine Learning Practitioner, I learned to code like everyone from the internet. . But I went in through difficulties and some roadblocks through the way, which even at times made me quit the idea of coding itself or somewhere it felt this isn’t for me. I have been there and I assume even people might felt that at some point isn’t? . Disclaimer: Everything you read here is absolutely out from my experience and my learnings. Some may disagree with it at times and take things as a grain of salt. . Why you should read this? . Well, because I don’t want you to waste your time looking for the right course or scratching your head at times waiting for the right thing. And even as a friend (if you consider). . I had a hard time choosing the courses or materials that could teach me the whole thing. But looking back, I was depending on courses and putting my faith in them. Courses are good, but at times we hit the tutorial hell. . Read my blog here to know more about whats Tutorial hell. . For now, it’s like a loop where we will be just finishing the course. Trust me, just because you did some course on Python, doesn’t mean you know it well. . Somehow people think the only way to learn to code is just by the means of taking different courses. Even I was there jumping from one course to another but it never improved my progress. . Note: I will be leaving up some links which helped me in my learning and eventually will do a favor for you too. . Don’t fall into the prey of Frameworks . When I was learning python for the first time, the motto was to get into Machine Learning for sure that’s obvious why most people choose python. . There were these frameworks popping out and it was the center of attraction, as tech enthusiasts I always wanted to equip myself with the best tools and especially the new ones. . This is where my shiny syndrome for frameworks started, I forget about learning python the way it should be (including OOP’s), most courses where I learned, the OOP’s concept was ignored. . But when I took the path of Experimenting Learning dumping down the tutorials, I was keen on working on my foundations. Learn things from documentation and more of research kinda way. I found it quite progressing when I tried on Pytorch, but this is where I felt like a complete noob. . While I was surfing through documentation and stuffs certain Python code doesn’t make sense to me, meaning hard to decode what’s written on there. . It was a complete noob-like feeling, I thought I knew python but things were not what I thought. . A fix in need is a fix indeed . I said above already, many people opt for the idea of learning from courses. There is no issue in doing that so, but it is when you fall into the loop. . How could we fix this? . Recently, I framed a thing called Experimental Learning, I watched video’s of other people on Youtube and observed how they did it. . Learning can be of two ways, one is we learn by Consuming something (courses, books, etc…) and another way is by Creating (trying to solve a problem, etc…). . When we mix these two wonderful components our learnings get better 10X times, but the problem is, we always spend more time on Consuming things. We should keep the balance 50–50 rather than weighing one side more. . Consuming . This is where you will be in the act of taking courses and reading books related to what programming langue you want to learn. But I see only fewer people opt for books, but I think books are really a good substitute for courses. . Stick with one course rather than too many, your goal should not be finishing the course and acquiring the certificate. Your purpose should be clear, you want to acquire this skill, not the certificate. . Creating . This is the fun part, where you will practice whatever you learned. Whatever I learn from the course at times couldn’t help me to solve real-world projects. . I end up hitting google and StackOverflow reading tons and tons of blogs and tutorials, read other’s code to find how others did it, and try reverse engineer them. You see there’s a lot of learning that goes in here. . In this way, we learn what we actually need to solve the problem rather than watching hours of tutorials with no clue whats the purpose. . The eye shrinks for you, which means you know what you want and your intent will be on finding just your need. . So now you might have an idea of whats Creating and Consuming really means, as I said by mixing them we could do wonders in our learning. . In the next part of this blog, will look into how we could mix these two pieces of stuff for enhancing our learning and useful courses and tidbits that could uplift your learning journey. . Here is what Part 2 of this blog contains: . Mixing Creation vs Consumption | Fixing the forgetting curve | Using spaced repetition for efficient learning. | Resources and course links!! | . (Check out now!)[#] .",
            "url": "https://ashikshafi08.github.io/fastpages/programming/2021/04/24/hwiltc_1.html",
            "relUrl": "/programming/2021/04/24/hwiltc_1.html",
            "date": " • Apr 24, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Intro to Maps, Filters and List Comprehension in Python",
            "content": "How to use Maps, Filters and List Comprehension in Python. . This notebook will covers the tutorial of map,filter and list comprehension. It&#39;s me learning these python goodies and documenting aside, so someday I could look up and revise stuffs. . I am going through the course on Coursera called Python-3 Programming from University of Michigan. Gotta admit I learnt alot of Python from there and still doing. Earlier I used to take notes on Notion, but now I thought of getting my hands dirty by coding along and take notes in Colab. . Also anyone interested or wanna refresh their Python skills could even make use of it. . Maps . Python provides built-in functions map and filter, even a new syntax called list comprehension that lets you express a mapping/filtering operation. Most documentations and programmers use list comprehension and it seem&#39;s more like a pythonic way of writing code. . Map, and filter are commands that you would use in high-performance computing on big datasets. http://en.wikipedia.org/wiki/MapReduce . . def doubleStuff(a_list): &#39;&#39;&#39; Returns a new list in which contains doubles of the elements in a list &#39;&#39;&#39; # Accum list new_list = [] # Looping through values and making the calculation for value in a_list: double_elem = 2 * value new_list.append(double_elem) return new_list # Using the above function a_list = [1 , 2, 3 , 4, 5 , 6] print(f&#39;List before the values were got double: {a_list}&#39;) double_list = doubleStuff(a_list) print(f&#39; nThe list after the values they got double the number: {double_list}&#39;) . List before the values were got double: [1, 2, 3, 4, 5, 6] The list after the values they got double the number: [2, 4, 6, 8, 10, 12] . We can write the above function with less than one line of code using the map function. . Map is a function which takes functions as the first input and sequence as an second input. map(function , sequence). Map just says apply the transforms (function) to every element in this sequence. . Map always expects a transformer function. . def double(value): return 2*value map_double_list = list(map(double , a_list)) print(f&#39;Using map function: {map_double_list}&#39;) . Using map function: [2, 4, 6, 8, 10, 12] . I earlier tried just map(double , a_list) which gave me just the map object. It turns out to be enclosing the map object by a list will gives us the list object. . But why this happens? Map function returns an iterator, it doesn&#39;t want to store the list in it&#39;s memory. It&#39;s still an iterator and we can grab the list we needed by enclosing the map object by a list which prevents pain to memory . . # Multiply 5 to every value map_lambda_list = list(map(lambda value: 5*value , a_list)) print(f&#39;Multiplying 5 to every element: {map_lambda_list}&#39;) . Multiplying 5 to every element: [5, 10, 15, 20, 25, 30] . . Let&#39;s Solve some Problems! . Gotta go through more: https://www.w3resource.com/python-exercises/map/index.php . Below we have provided a list of strings called abbrevs. Use map to produce a new list called abbrevs_upper that contains all the same strings in upper case. | abbrevs = [&#39;usa&#39; , &#39;esp&#39; , &#39;chn&#39; , &#39;jpn&#39; , &#39;mex&#39; , &#39;can&#39; , &#39;rus&#39; , &#39;rsa&#39; , &#39;jam&#39;] . upperAbbrev_list = [] for abbrev in abbrevs: upperAbbrev_list.append(abbrev.upper()) print(upperAbbrev_list) . [&#39;USA&#39;, &#39;ESP&#39;, &#39;CHN&#39;, &#39;JPN&#39;, &#39;MEX&#39;, &#39;CAN&#39;, &#39;RUS&#39;, &#39;RSA&#39;, &#39;JAM&#39;] . upperCase_abbrevs = list(map(lambda abbrev: abbrev.upper() , abbrevs)) print(upperCase_abbrevs) . [&#39;USA&#39;, &#39;ESP&#39;, &#39;CHN&#39;, &#39;JPN&#39;, &#39;MEX&#39;, &#39;CAN&#39;, &#39;RUS&#39;, &#39;RSA&#39;, &#39;JAM&#39;] . Using map, create a list assigned to the variable greeting_doubled that doubles each element in the list. | lst = [[&quot;hi&quot;, &quot;bye&quot;], &quot;hello&quot;, &quot;goodbye&quot;, [9, 2], 4] . lst = [[&quot;hi&quot;, &quot;bye&quot;], &quot;hello&quot;, &quot;goodbye&quot;, [9, 2], 4] greeting_doubled = list(map(lambda element: 2 * element , lst)) print(greeting_doubled) . [[&#39;hi&#39;, &#39;bye&#39;, &#39;hi&#39;, &#39;bye&#39;], &#39;hellohello&#39;, &#39;goodbyegoodbye&#39;, [9, 2, 9, 2], 8] . Write a Python program to add three given lists using Python map and lambda | list(map(lambda a,b,c: a + b + c , [1 , 2 ,3] , [4 ,5 , 6] , [7 ,8 , 9])) . [12, 15, 18] . Filters . Filter function filter takes two arguments same like our map which has both function and a sequence parameters. Instead mapping them or making calculation with eachother, filter filters out the numbers either True or False. . The function takes one item and return True if the item should. It is automatically called for each item in the sequence . filter returns an iterator object like map, so we gotta wrap them by list. . def keep_evens(a_list): new_list = [] for elem in a_list: if elem % 2 == 0: new_list.append(elem) return new_list # Using the above function mixList = [2 , 88 , 33 , 22 , 14 , 0 , 8 , 10 , 20 , 4] evenList = keep_evens(mixList) print(evenList) . [2, 88, 22, 14, 0, 8, 10, 20, 4] . filterEvenList = list(filter(lambda elem: elem % 2 == 0 , mixList)) print(filterEvenList) . [2, 88, 22, 14, 0, 8, 10, 20, 4] . Let&#39;s Solve some Problems . Using filter, filter lst so that it only contains words containing the letter &#39;o&#39;. Assign to variable lst2. | lst = [&#39;witch&#39; , &#39;halloween&#39; , &#39;pumpkin&#39; , &#39;cat&#39; , &#39;candy&#39; , &#39;wagon&#39; ,&#39;moon&#39;] . lst = [&#39;witch&#39; , &#39;halloween&#39; , &#39;pumpkin&#39; , &#39;cat&#39; , &#39;candy&#39; , &#39;wagon&#39; , &#39;moon&#39;] lst2 = list(filter(lambda elem: &#39;o&#39; in elem , lst)) print(lst2) . [&#39;halloween&#39;, &#39;wagon&#39;, &#39;moon&#39;] . Write code to assign to the variable filter_testing all the elements in lst_check that have a &#39;w&#39; in them using filter. | lst_check = [&#39;plums&#39;, &#39;watermelon&#39;, &#39;kiwi&#39;, &#39;strawberries&#39;, &#39;blueberries&#39;, &#39;peaches&#39;, &#39;apples&#39;, &#39;mangos&#39;, &#39;papaya&#39;] . lst_check = [&#39;plums&#39;, &#39;watermelon&#39;, &#39;kiwi&#39;, &#39;strawberries&#39;, &#39;blueberries&#39;, &#39;peaches&#39;, &#39;apples&#39;, &#39;mangos&#39;, &#39;papaya&#39;] filter_testing = list(filter(lambda word: &#39;w&#39; in word , lst_check)) print(filter_testing) . [&#39;watermelon&#39;, &#39;kiwi&#39;, &#39;strawberries&#39;] . List Comprehensions . Before we saw those two functions namely map and filter turns out to be we don&#39;t want to use them much (or) in other words we can use list comprehensions inplace of using map and filter. Better we can pull off more flexibility by using list comprehensions. . In simple words list comprehensions is a convinient syntax to do map and filter operations. . Basic Syntax of list comprehension: . [ &lt;transformer_expression&gt; for &lt;iterator_variable&gt; in &lt;sequence&gt; if &lt;filteration_expression&gt;] . def double(value): return 2*value map_double_list = list(map(double , a_list)) print(f&#39;Using map function: {map_double_list}&#39;) . Using map function: [2, 4, 6, 8, 10, 12] . a_list . [1, 2, 3, 4, 5, 6] . compre_double_list = [value * 2 for value in a_list] compre_double_list . [2, 4, 6, 8, 10, 12] . Breaking down by the syntax: . transformer_expression : value * 2 | iterator_varaible : value | sequence : a_list | . filterEvenList = list(filter(lambda elem: elem % 2 == 0 , mixList)) print(filterEvenList) . [2, 88, 22, 14, 0, 8, 10, 20, 4] . mixList . [2, 88, 33, 22, 14, 0, 8, 10, 20, 4] . filterListComprehension = [element for element in mixList if element % 2 == 0] filterListComprehension . [2, 88, 22, 14, 0, 8, 10, 20, 4] . filterListComprehension == filterEvenList . True . Write code to assign to the variable compri all the values of the key name in any of the sub-dictionaries in the dictionary tester. Do this using a list comprehension. . tester = {&#39;info&#39;: [{&quot;name&quot;: &quot;Lauren&quot;, &#39;class standing&#39;: &#39;Junior&#39;, &#39;major&#39;: &quot;Information Science&quot;},{&#39;name&#39;: &#39;Ayo&#39;, &#39;class standing&#39;: &quot;Bachelor&#39;s&quot;, &#39;major&#39;: &#39;Information Science&#39;}, {&#39;name&#39;: &#39;Kathryn&#39;, &#39;class standing&#39;: &#39;Senior&#39;, &#39;major&#39;: &#39;Sociology&#39;}, {&#39;name&#39;: &#39;Nick&#39;, &#39;class standing&#39;: &#39;Junior&#39;, &#39;major&#39;: &#39;Computer Science&#39;}, {&#39;name&#39;: &#39;Gladys&#39;, &#39;class standing&#39;: &#39;Sophomore&#39;, &#39;major&#39;: &#39;History&#39;}, {&#39;name&#39;: &#39;Adam&#39;, &#39;major&#39;: &#39;Violin Performance&#39;, &#39;class standing&#39;: &#39;Senior&#39;}]} inner_list = tester[&#39;info&#39;] #print(inner_list) # For Readability import json print(json.dumps(inner_list , indent = 2)) . [ { &#34;name&#34;: &#34;Lauren&#34;, &#34;class standing&#34;: &#34;Junior&#34;, &#34;major&#34;: &#34;Information Science&#34; }, { &#34;name&#34;: &#34;Ayo&#34;, &#34;class standing&#34;: &#34;Bachelor&#39;s&#34;, &#34;major&#34;: &#34;Information Science&#34; }, { &#34;name&#34;: &#34;Kathryn&#34;, &#34;class standing&#34;: &#34;Senior&#34;, &#34;major&#34;: &#34;Sociology&#34; }, { &#34;name&#34;: &#34;Nick&#34;, &#34;class standing&#34;: &#34;Junior&#34;, &#34;major&#34;: &#34;Computer Science&#34; }, { &#34;name&#34;: &#34;Gladys&#34;, &#34;class standing&#34;: &#34;Sophomore&#34;, &#34;major&#34;: &#34;History&#34; }, { &#34;name&#34;: &#34;Adam&#34;, &#34;major&#34;: &#34;Violin Performance&#34;, &#34;class standing&#34;: &#34;Senior&#34; } ] . nameList = [] if True: for dict_name in inner_list: name = dict_name[&#39;name&#39;] nameList.append(name) print(nameList) . [&#39;Lauren&#39;, &#39;Ayo&#39;, &#39;Kathryn&#39;, &#39;Nick&#39;, &#39;Gladys&#39;, &#39;Adam&#39;] . compri = [dict_value[&#39;name&#39;] for dict_value in inner_list if True] compri . [&#39;Lauren&#39;, &#39;Ayo&#39;, &#39;Kathryn&#39;, &#39;Nick&#39;, &#39;Gladys&#39;, &#39;Adam&#39;] .",
            "url": "https://ashikshafi08.github.io/fastpages/jupyter/2021/04/11/map-filter.html",
            "relUrl": "/jupyter/2021/04/11/map-filter.html",
            "date": " • Apr 11, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hey it’s Ashik here! I ma Machine Learning Practitioner. .",
          "url": "https://ashikshafi08.github.io/fastpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Challenge Logs",
          "content": "I will be logging my activities here during the challenge . Test . Testing things out to make sure will this work or not Watching TensorFlow stream by my friend lol So we can stack up these bulletins that’s cool! | . | . | .",
          "url": "https://ashikshafi08.github.io/fastpages/challenge_logs/",
          "relUrl": "/challenge_logs/",
          "date": ""
      }
      
  

  

  
  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ashikshafi08.github.io/fastpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}