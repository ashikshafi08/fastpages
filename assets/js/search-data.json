{
  
    
        "post0": {
            "title": "Tensorflow Deeplearning",
            "content": "Zero to Mastery Deep Learning with TensorFlow (coming VERY soon, stay tuned for updates) . All of the course materials for the Zero to Mastery Deep Learning with TensorFlow course. . This course will teach you foundations of deep learning and TensorFlow as well as prepare you to pass the TensorFlow Developer Certification exam (optional). . Contents of this page: . Course materials (everything you‚Äôll need for completing the course) | Course structure (how this course is taught) | Should you do this course? (decide by answering a couple simple questions) | Prerequisites (what skills you‚Äôll need to do this course) | Exercises &amp; Extra-curriculum (challenges to practice what you‚Äôve learned and resources to learn more) | Ask a question (like to know more? go here) | Status/TODO (there‚Äôs still more to come!) | Log (updates, changes and progress) | . Course materials . This table is the ground truth for course materials. All the links you need for everything will be here. . Key: . Number: The number of the target notebook (this may not match the video section of the course but it ties together all of the materials in the table) | Notebook: The notebook for a particular module with lots of code and text annotations (notebooks from the videos are based on these) | Data/model: Links to datasets/pre-trained models for the assosciated notebook | Exercises &amp; Extra-curriculum: Each module comes with a set of exercises and extra-curriculum to help practice your skills and learn more, I suggest going through these before you move onto the next module | Slides: Although we focus on writing TensorFlow code, we sometimes use pretty slides to describe different concepts, you‚Äôll find them here | . Number Notebook Data/Model Exercises &amp; Extra-curriculum Slides . 00 | TensorFlow Fundamentals | ¬† | Go to exercises &amp; extra-curriculum | Go to slides | . 01 | TensorFlow Regression | ¬† | Go to exercises &amp; extra-curriculum | Go to slides | . 02 | TensorFlow Classification | ¬† | Go to exercises &amp; extra-curriculum | Go to slides | . 03 | TensorFlow Computer Vision | pizza_steak, 10_food_classes_all_data | Go to exercises &amp; extra-curriculum | Go to slides | . 04 | Transfer Learning Part 1: Feature extraction | 10_food_classes_10_percent | Go to exercises &amp; extra-curriculum | Go to slides | . 05 | Transfer Learning Part 2: Fine-tuning | 10_food_classes_10_percent, 10_food_classes_1_percent, 10_food_classes_all_data | Go to exercises &amp; extra-curriculum | Go to slides | . 06 | Transfer Learning Part 3: Scaling up | 101_food_classes_10_percent, custom_food_images, fine_tuned_efficientnet_model | Go to exercises &amp; extra-curriculum | Coming soon | . 07 | Milestone project 1 (coming soon) | ¬† | ¬† | ¬† | . 08 | TensorFlow NLP Fundamentals (coming soon) | ¬† | ¬† | ¬† | . 09 | Milestone project 2 (coming soon) | ¬† | ¬† | ¬† | . 10 | TensorFlow Time Series Fundamentals &amp; Milestone project 3 (coming soon) | ¬† | ¬† | ¬† | . Course structure . This course is code first. The goal is to get you writing deep learning code as soon as possible. . It is taught with the following mantra: . Code -&gt; Concept -&gt; Code -&gt; Concept -&gt; Code -&gt; Concept . This means we write code first then step through the concepts behind it. . If you‚Äôve got 6-months experience writing Python code and a willingness to learn (most important), you‚Äôll be able to do the course. . Should you do this course? . Do you have 1+ years experience with deep learning and writing TensorFlow code? . If yes, no you shouldn‚Äôt, use your skills to build something. . If no, move onto the next question. . Have you done at least one beginner machine learning course and would like to learn about deep learning/pass the TensorFlow Developer Certification? . If yes, this course is for you. . If no, go and do a beginner machine learning course and if you decide you want to learn TensorFlow, this page will still be here. . Prerequisites . What do I need to know to go through this course? . 6+ months writing Python code. Can you write a Python function which accepts and uses parameters? That‚Äôs good enough. If you don‚Äôt know what that means, spend another month or two writing Python code and then come back here. | At least one beginner machine learning course. Are you familiar with the idea of training, validation and test sets? Do you know what supervised learning is? Have you used pandas, NumPy or Matplotlib before? If no to any of these, I‚Äôd going through at least one machine learning course which teaches these first and then coming back. | Comfortable using Google Colab/Jupyter Notebooks. This course uses Google Colab throughout. If you have never used Google Colab before, it works very similar to Jupyter Notebooks with a few extra features. If you‚Äôre not familiar with Google Colab notebooks, I‚Äôd suggest going through the Introduction to Google Colab notebook. | Plug: The Zero to Mastery beginner-friendly machine learning course (I also teach this) teaches all of the above (and this course is designed as a follow on). | . üõ† Exercises &amp; üìñ Extra-curriculum . To prevent the course from being 100+ hours (deep learning is a broad field), various external resources for different sections are recommended to puruse under your own discrestion. . (solutions to come after the course is released‚Ä¶ try the exercises out for yourself first!) . . üõ† 00 TensorFlow Fundamentals Exercises . Create a vector, scalar, matrix and tensor with values of your choosing using tf.constant(). | Find the shape, rank and size of the tensors you created in 1. | Create two tensors containing random values between 0 and 1 with shape [5, 300]. | Multiply the two tensors you created in 3 using matrix multiplication. | Multiply the two tensors you created in 3 using dot product. | Create a tensor with random values between 0 and 1 with shape [224, 224, 3]. | Find the min and max values of the tensor you created in 6. | Created a tensor with random values of shape [1, 224, 224, 3] then squeeze it to change the shape to [224, 224, 3]. | Create a tensor with shape [10] using your own choice of values, then find the index which has the maximum value. | One-hot encode the tensor you created in 9. | üìñ 00 TensorFlow Fundamentals Extra-curriculum . Read through the list of TensorFlow Python APIs, pick one we haven‚Äôt gone through in this notebook, reverse engineer it (write out the documentation code for yourself) and figure out what it does. | Try to create a series of tensor functions to calculate your most recent grocery bill (it‚Äôs okay if you don‚Äôt use the names of the items, just the price in numerical form). How would you calculate your grocery bill for the month and for the year using tensors? | . | Go through the TensorFlow 2.x quick start for beginners tutorial (be sure to type out all of the code yourself, even if you don‚Äôt understand it). Are there any functions we used in here that match what‚Äôs used in there? Which are the same? Which haven‚Äôt you seen before? | . | Watch the video ‚ÄúWhat‚Äôs a tensor?‚Äù - a great visual introduction to many of the concepts we‚Äôve covered in this notebook. | . . üõ† 01 Neural network regression with TensorFlow Exercises . Create your own regression dataset (or make the one we created in ‚ÄúCreate data to view and fit‚Äù bigger) and build fit a model to it. | Try building a neural network with 4 Dense layers and fitting it to your own regression dataset, how does it perform? | Try and improve the results we got on the insurance dataset, some things you might want to try include: Building a larger model (how does one with 4 dense layers go?). | Increasing the number of units in each layer. | Lookup the documentation of Adam and find out what the first parameter is, what happens if you increase it by 10x? | What happens if you train for longer (say 300 epochs instead of 200)? | . | Import the Boston pricing dataset from TensorFlow tf.keras.datasets and model it. | üìñ 01 Neural network regression with TensorFlow Extra-curriculum . MIT introduction deep learning lecture 1 - gives a great overview of what‚Äôs happening behind all of the code we‚Äôre running. | Reading: 1-hour of Chapter 1 of Neural Networks and Deep Learning by Michael Nielson - a great in-depth and hands-on example of the intuition behind neural networks. | To practice your regression modelling with TensorFlow, I‚Äôd also encourage you to look through Lion Bridge‚Äôs collection of datasets or Kaggle‚Äôs datasets, find a regression dataset which sparks your interest and try to model. | . . üõ† 02 Neural network classification with TensorFlow Exercises . Play with neural networks in the TensorFlow Playground for 10-minutes. Especially try different values of the learning, what happens when you decrease it? What happens when you increase it? | Replicate the model pictured in the TensorFlow Playground diagram below using TensorFlow code. Compile it using the Adam optimizer, binary crossentropy loss and accuracy metric. Once it‚Äôs compiled check a summary of the model. Try this network out for yourself on the TensorFlow Playground website. Hint: there are 5 hidden layers but the output layer isn‚Äôt pictured, you‚Äôll have to decide what the output layer should be based on the input data. | Create a classification dataset using Scikit-Learn‚Äôs make_moons() function, visualize it and then build a model to fit it at over 85% accuracy. | Create a function (or write code) to visualize multiple image predictions for the fashion MNIST at the same time. Plot at least three different images and their prediciton labels at the same time. Hint: see the classifcation tutorial in the TensorFlow documentation for ideas. | Recreate TensorFlow‚Äôs softmax activation function in your own code. Make sure it can accept a tensor and return that tensor after having the softmax function applied to it. | Train a model to get 88%+ accuracy on the fashion MNIST test set. Plot a confusion matrix to see the results after. | Make a function to show an image of a certain class of the fashion MNIST dataset and make a prediction on it. For example, plot 3 images of the T-shirt class with their predictions. | üìñ 02 Neural network classification with TensorFlow Extra-curriculum . Watch 3Blue1Brown‚Äôs neural networks video 2: Gradient descent, how neural networks learn. After you‚Äôre done, write 100 words about what you‚Äôve learned. If you haven‚Äôt already, watch video 1: But what is a Neural Network?. Note the activation function they talk about at the end. | . | Watch MIT‚Äôs introduction to deep learning lecture 1 (if you haven‚Äôt already) to get an idea of the concepts behind using linear and non-linear functions. | Spend 1-hour reading Michael Nielsen‚Äôs Neural Networks and Deep Learning book. | Read the ML-Glossary documentation on activation functions. Which one is your favourite? After you‚Äôve read the ML-Glossary, see which activation functions are available in TensorFlow by searching ‚Äútensorflow activation functions‚Äù. | . | . . üõ† 03 Computer vision &amp; convolutional neural networks in TensorFlow Exercises . Spend 20-minutes reading and interacting with the CNN explainer website. What are the key terms? e.g. explain convolution in your own words, pooling in your own words | . | Play around with the ‚Äúunderstanding hyperparameters‚Äù section in the CNN explainer website for 10-minutes. What is the kernel size? | What is the stride? | How could you adjust each of these in TensorFlow code? | . | Take 10 photos of two different things and build your own CNN image classifier using the techniques we‚Äôve built here. | Find an ideal learning rate for a simple convolutional neural network model on your the 10 class dataset. | üìñ 03 Computer vision &amp; convolutional neural networks in TensorFlow Extra-curriculum . Watch: MIT‚Äôs Introduction to Deep Computer Vision lecture. This will give you a great intuition behind convolutional neural networks. | Watch: Deep dive on mini-batch gradient descent by deeplearning.ai. If you‚Äôre still curious about why we use batches to train models, this technical overview covers many of the reasons why. | Read: CS231n Convolutional Neural Networks for Visual Recognition class notes. This will give a very deep understanding of what‚Äôs going on behind the scenes of the convolutional neural network architectures we‚Äôre writing. | Read: ‚ÄúA guide to convolution arithmetic for deep learning‚Äù. This paper goes through all of the mathematics running behind the scenes of our convolutional layers. | Code practice: TensorFlow Data Augmentation Tutorial. For a more in-depth introduction on data augmentation with TensorFlow, spend an hour or two reading through this tutorial. | . . üõ† 04 Transfer Learning in TensorFlow Part 1: Feature Extraction Exercises . Build and fit a model using the same data we have here but with the MobileNetV2 architecture feature extraction (mobilenet_v2_100_224/feature_vector) from TensorFlow Hub, how does it perform compared to our other models? | Name 3 different image classification models on TensorFlow Hub that we haven‚Äôt used. | Build a model to classify images of two different things you‚Äôve taken photos of. You can use any feature extraction layer from TensorFlow Hub you like for this. | You should aim to have at least 10 images of each class, for example to build a fridge versus oven classifier, you‚Äôll want 10 images of fridges and 10 images of ovens. | . | What is the current best performing model on ImageNet? Hint: you might want to check sotabench.com for this. | . | üìñ 04 Transfer Learning in TensorFlow Part 1: Feature Extraction Extra-curriculum . Read through the TensorFlow Transfer Learning Guide and define the main two types of transfer learning in your own words. | Go through the Transfer Learning with TensorFlow Hub tutorial on the TensorFlow website and rewrite all of the code yourself into a new Google Colab notebook making comments about what each step does along the way. | We haven‚Äôt covered fine-tuning with TensorFlow Hub in this notebook, but if you‚Äôd like to know more, go through the fine-tuning a TensorFlow Hub model tutorial on the TensorFlow homepage.How to fine-tune a tensorflow hub model: | Look into experiment tracking with Weights &amp; Biases, how could you integrate it with our existing TensorBoard logs? | . . üõ† 05 Transfer Learning in TensorFlow Part 2: Fine-tuning Exercises . Write a function to visualize an image from any dataset (train or test file) and any class (e.g. ‚Äústeak‚Äù, ‚Äúpizza‚Äù‚Ä¶ etc), visualize it and make a prediction on it using a trained model. | Use feature-extraction to train a transfer learning model on 10% of the Food Vision data for 10 epochs using tf.keras.applications.EfficientNetB0 as the base model. Use the ModelCheckpoint callback to save the weights to file. | Fine-tune the last 20 layers of the base model you trained in 2 for another 10 epochs. How did it go? | Fine-tune the last 30 layers of the base model you trained in 2 for another 10 epochs. How did it go? | üìñ 05 Transfer Learning in TensorFlow Part 2: Fine-tuning Extra-curriculum . Read the documentation on data augmentation in TensorFlow. | Read the ULMFit paper (technical) for an introduction to the concept of freezing and unfreezing different layers. | Read up on learning rate scheduling (there‚Äôs a TensorFlow callback for this), how could this influence our model training? If you‚Äôre training for longer, you probably want to reduce the learning rate as you go‚Ä¶ the closer you get to the bottom of the hill, the smaller steps you want to take. Imagine it like finding a coin at the bottom of your couch. In the beginning your arm movements are going to be large and the closer you get, the smaller your movements become. | . | . . üõ† 06 Transfer Learning in TensorFlow Part 3: Scaling-up Exercises . Take 3 of your own photos of food and use the trained model to make predictions on them, share your predictions with the other students in Discord and show off your Food Vision model üçîüëÅ. | Train a feature-extraction transfer learning model for 10 epochs on the same data and compare its performance versus a model which used feature extraction for 5 epochs and fine-tuning for 5 epochs (like we‚Äôve used in this notebook). Which method is better? | üìñ 06 Transfer Learning in TensorFlow Part 3: Scaling-up Extra-curriculum . Spend 15-minutes reading up on the EarlyStopping callback. What does it do? How could we use it in our model training? | Spend an hour reading about Streamlit. What does it do? How might you integrate some of the things we‚Äôve done in this notebook in a Streamlit app? | . . What this course is missing . Transformers | Multi-modal models | . Ask questions . Contact Daniel Bourke or add a discussion (preferred). . Status . As of: 23 Feb 2021 . Currently: Preparing to record videos for 06, adding resources/cleaning GitHub | Video count: 160/~220+, aiming to do ~10 videos per day during recording sessions | Finished videos for: 00, 01, 02, 03, 04, 05 | Finished slides for notebooks: 00, 01, 02, 03, 04, 05, 06 | Polished (prepared them for external use) notebooks: 00, 01, 02, 03, 04, 05, 06 | Finished 09/10 of code notebooks (time series still to come) | Video studio setup! (see the makeshift closet studio) | Created GitHub Project page! See a cool Kanban setup here: https://github.com/mrdbourke/tensorflow-deep-learning/projects/1 | Created GitHub Discussion page! Going to use this for popular QA/course tidbits: https://github.com/mrdbourke/tensorflow-deep-learning/discussions | . TODO . geez‚Ä¶ I forgot how much there was still to go‚Ä¶ classic project planning . ‚úÖ Make a GitHub Project for course (see ‚ÄúProjects‚Äù tab) | ‚úÖ Make a GitHub Discussions for course (thank you Alvaro) | ‚úÖ Polish GitHub readme (what you‚Äôre reading now) with extra resources: data links used in course | extra resources &amp; curriculum | . | üîú Upload slides for each section, done for: 00, 01, 02, 03, 04, 05 (see course materials) | üîú Upload video notebooks for each section, done for: 00, 01, 02, 03, 04, 05 (see .video_notebooks/) | Add a section which contains ‚Äúthings taught in this course‚Äù, like a table of contents kind of thing | Make Colab overview video (Colab is the tool we‚Äôll be using for the whole course) | Make course resource overview video (e.g. how to use this GitHub, Discussions page, exercises, extra-curriculum etc) | Upload solutions for exercises (probably livestream the creation of these after course launch) | . Log . 23 Feb 2021 - rearranged GitHub in preparation for launch üöÄ | 18 Feb 2021 - recorded 8 videos for 05 and‚Ä¶ it‚Äôs done! onto polishing the GitHub | 17 Feb 2021 - recorded 10 videos for 05! going to finish tomorrow üöÄ | 16 Feb 2021 - polished slides for 05 and started recording videos, got 7 videos done for 05 | 15 Feb 2021 - finished videos for 04, now preparing to record for 05! | 12 Feb 2021 - recored 7 videos for section 04‚Ä¶ wanted 10 but we‚Äôll take 7 (ü§î this seems to have happened before) | 11 Feb 2021 - NO PROGRESS - gave a Machine Learning deployment tutorial for Stanford‚Äôs CS329s (using the model code from this course!!!) - see the full tutorial materials | 08 Feb 2021 - recorded 10 videos for section 03‚Ä¶ and section 03 is done! üöÄ onto section 04 | 30 Jan 2021 - 07 Feb 2021: NO PROGRESS (working on a ML deployment lecture for Stanford‚Äôs CS329s‚Ä¶ more on this later) | 29 Jan 2021 - recorded 9 videos for section 03‚Ä¶ closer to 10 than yesterday but still not there | 28 Jan 2021 - recorded 7 videos for section 03‚Ä¶ wanted 10 but we‚Äôll take 7 | 27 Jan 2021 - recorded 10 videos for section 03 | 26 Jan 2021 - polished GitHub README (what you‚Äôre looking at) with a nice table | 23 Jan 2021 - finished slides of 06 | 22 Jan 2021 - finished review of notebook 06 &amp; started slides of 06 | 21 Jan 2021 - finished slides for 05 &amp; started review of 06 | 20 Jan 2021 - finished notebook 05 &amp; 95% slides for 05 | 19 Jan 2021 - found a storage idea for data during course (use Google Storage in same region as Colab Notebooks, cheapest/fastest) | 18 Jan 2021 - reviewed notebook 05 &amp; slides for 05 | 17 Jan 2021 - finished notebook 04 &amp; slides for 04 | 16 Jan 2021 - review notebook 04 &amp; made slides for transfer learning | 13 Jan 2021 - review notebook 03 again &amp; finished slides for 03, BIGGGGG updates to the README, notebook 03 99% done, just need to figure out optimum way to transfer data (e.g. when a student downloads it, where‚Äôs best to store it in the meantime? Dropbox? S3? GS (too expensive) | 11 Jan 2021 - reviewed notebook 03, 95% ready for recording, onto slides for 03 | 9 Jan 2021 - I‚Äôm back baby! Finished all videos for 02, now onto slides/materials for 03, 04, 05 (then I‚Äôll get back in the lab) | 19 Dec 2020 - ON HOLD (family holiday until Jan 02 2021) | 18 Dec 2020 - recorded 75% of videos for 02 | 17 Dec 2020 - recorded 50% of videos for 02 | 16 Dec 2020 - recorded 100% of videos for 01 | 15 Dec 2020 - recorded 90% of videos for 01 | 09 Dec 2020 - finished recording videos for 00 | 08 Dec 2020 - recorded 90% of videos for 00 | 05 Dec 2020 - trialled recording studio for ~6 videos with notebook 00 material | 04 Dec 2020 - setup recording studio in closet | 03 Dec 2020 - finished notebook 02, finished slides for 02, time to setup recording studio | 02 Dec 2020 - notebook 02 95% done, slides for 02 90% done | 01 Dec 2020 - added notebook 02 (90% polished), start preparing slides for 02 | 27 Nov 2020 - polished notebook 01, made slides for notebook 01 | 26 Nov 2020 - polished notebook 00, made slides for notebook 00 | .",
            "url": "https://ashikshafi08.github.io/fastpages/2021/04/12/tensorflow-deeplearning.html",
            "relUrl": "/2021/04/12/tensorflow-deeplearning.html",
            "date": " ‚Ä¢ Apr 12, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Intro to Maps, Filters and List Comprehension in Python",
            "content": "How to use Maps, Filters and List Comprehension in Python. . This notebook will covers the tutorial of map,filter and list comprehension. It&#39;s me learning these python goodies and documenting aside, so someday I could look up and revise stuffs. . I am going through the course on Coursera called Python-3 Programming from University of Michigan. Gotta admit I learnt alot of Python from there and still doing. Earlier I used to take notes on Notion, but now I thought of getting my hands dirty by coding along and take notes in Colab. . Also anyone interested or wanna refresh their Python skills could even make use of it. . Maps . Python provides built-in functions map and filter, even a new syntax called list comprehension that lets you express a mapping/filtering operation. Most documentations and programmers use list comprehension and it seem&#39;s more like a pythonic way of writing code. . Map, and filter are commands that you would use in high-performance computing on big datasets. http://en.wikipedia.org/wiki/MapReduce . def doubleStuff(a_list): &#39;&#39;&#39; Returns a new list in which contains doubles of the elements in a list &#39;&#39;&#39; # Accum list new_list = [] # Looping through values and making the calculation for value in a_list: double_elem = 2 * value new_list.append(double_elem) return new_list # Using the above function a_list = [1 , 2, 3 , 4, 5 , 6] print(f&#39;List before the values were got double: {a_list}&#39;) double_list = doubleStuff(a_list) print(f&#39; nThe list after the values they got double the number: {double_list}&#39;) . List before the values were got double: [1, 2, 3, 4, 5, 6] The list after the values they got double the number: [2, 4, 6, 8, 10, 12] . We can write the above function with less than one line of code using the map function. . Map is a function which takes functions as the first input and sequence as an second input. map(function , sequence). Map just says apply the transforms (function) to every element in this sequence. . Map always expects a transformer function. . def double(value): return 2*value map_double_list = list(map(double , a_list)) print(f&#39;Using map function: {map_double_list}&#39;) . Using map function: [2, 4, 6, 8, 10, 12] . I earlier tried just map(double , a_list) which gave me just the map object. It turns out to be enclosing the map object by a list will gives us the list object. . But why this happens? Map function returns an iterator, it doesn&#39;t want to store the list in it&#39;s memory. It&#39;s still an iterator and we can grab the list we needed by enclosing the map object by a list which prevents pain to memory . . # Multiply 5 to every value map_lambda_list = list(map(lambda value: 5*value , a_list)) print(f&#39;Multiplying 5 to every element: {map_lambda_list}&#39;) . Multiplying 5 to every element: [5, 10, 15, 20, 25, 30] . Let&#39;s Solve some Problems! . Gotta go through more: https://www.w3resource.com/python-exercises/map/index.php . Below we have provided a list of strings called abbrevs. Use map to produce a new list called abbrevs_upper that contains all the same strings in upper case. | abbrevs = [&#39;usa&#39; , &#39;esp&#39; , &#39;chn&#39; , &#39;jpn&#39; , &#39;mex&#39; , &#39;can&#39; , &#39;rus&#39; , &#39;rsa&#39; , &#39;jam&#39;] . upperAbbrev_list = [] for abbrev in abbrevs: upperAbbrev_list.append(abbrev.upper()) print(upperAbbrev_list) . [&#39;USA&#39;, &#39;ESP&#39;, &#39;CHN&#39;, &#39;JPN&#39;, &#39;MEX&#39;, &#39;CAN&#39;, &#39;RUS&#39;, &#39;RSA&#39;, &#39;JAM&#39;] . upperCase_abbrevs = list(map(lambda abbrev: abbrev.upper() , abbrevs)) print(upperCase_abbrevs) . [&#39;USA&#39;, &#39;ESP&#39;, &#39;CHN&#39;, &#39;JPN&#39;, &#39;MEX&#39;, &#39;CAN&#39;, &#39;RUS&#39;, &#39;RSA&#39;, &#39;JAM&#39;] . Using map, create a list assigned to the variable greeting_doubled that doubles each element in the list. | lst = [[&quot;hi&quot;, &quot;bye&quot;], &quot;hello&quot;, &quot;goodbye&quot;, [9, 2], 4] . lst = [[&quot;hi&quot;, &quot;bye&quot;], &quot;hello&quot;, &quot;goodbye&quot;, [9, 2], 4] greeting_doubled = list(map(lambda element: 2 * element , lst)) print(greeting_doubled) . [[&#39;hi&#39;, &#39;bye&#39;, &#39;hi&#39;, &#39;bye&#39;], &#39;hellohello&#39;, &#39;goodbyegoodbye&#39;, [9, 2, 9, 2], 8] . Write a Python program to add three given lists using Python map and lambda | list(map(lambda a,b,c: a + b + c , [1 , 2 ,3] , [4 ,5 , 6] , [7 ,8 , 9])) . [12, 15, 18] . Filters . Filter function filter takes two arguments same like our map which has both function and a sequence parameters. Instead mapping them or making calculation with eachother, filter filters out the numbers either True or False. . The function takes one item and return True if the item should. It is automatically called for each item in the sequence . filter returns an iterator object like map, so we gotta wrap them by list. . def keep_evens(a_list): new_list = [] for elem in a_list: if elem % 2 == 0: new_list.append(elem) return new_list # Using the above function mixList = [2 , 88 , 33 , 22 , 14 , 0 , 8 , 10 , 20 , 4] evenList = keep_evens(mixList) print(evenList) . [2, 88, 22, 14, 0, 8, 10, 20, 4] . filterEvenList = list(filter(lambda elem: elem % 2 == 0 , mixList)) print(filterEvenList) . [2, 88, 22, 14, 0, 8, 10, 20, 4] . Let&#39;s Solve some Problems . Using filter, filter lst so that it only contains words containing the letter &#39;o&#39;. Assign to variable lst2. | lst = [&#39;witch&#39; , &#39;halloween&#39; , &#39;pumpkin&#39; , &#39;cat&#39; , &#39;candy&#39; , &#39;wagon&#39; ,&#39;moon&#39;] . lst = [&#39;witch&#39; , &#39;halloween&#39; , &#39;pumpkin&#39; , &#39;cat&#39; , &#39;candy&#39; , &#39;wagon&#39; , &#39;moon&#39;] lst2 = list(filter(lambda elem: &#39;o&#39; in elem , lst)) print(lst2) . [&#39;halloween&#39;, &#39;wagon&#39;, &#39;moon&#39;] . Write code to assign to the variable filter_testing all the elements in lst_check that have a &#39;w&#39; in them using filter. | lst_check = [&#39;plums&#39;, &#39;watermelon&#39;, &#39;kiwi&#39;, &#39;strawberries&#39;, &#39;blueberries&#39;, &#39;peaches&#39;, &#39;apples&#39;, &#39;mangos&#39;, &#39;papaya&#39;] . lst_check = [&#39;plums&#39;, &#39;watermelon&#39;, &#39;kiwi&#39;, &#39;strawberries&#39;, &#39;blueberries&#39;, &#39;peaches&#39;, &#39;apples&#39;, &#39;mangos&#39;, &#39;papaya&#39;] filter_testing = list(filter(lambda word: &#39;w&#39; in word , lst_check)) print(filter_testing) . [&#39;watermelon&#39;, &#39;kiwi&#39;, &#39;strawberries&#39;] . List Comprehensions . Before we saw those two functions namely map and filter turns out to be we don&#39;t want to use them much (or) in other words we can use list comprehensions inplace of using map and filter. Better we can pull off more flexibility by using list comprehensions. . In simple words list comprehensions is a convinient syntax to do map and filter operations. . Basic Syntax of list comprehension: . [ &lt;transformer_expression&gt; for &lt;iterator_variable&gt; in &lt;sequence&gt; if &lt;filteration_expression&gt;] . def double(value): return 2*value map_double_list = list(map(double , a_list)) print(f&#39;Using map function: {map_double_list}&#39;) . Using map function: [2, 4, 6, 8, 10, 12] . a_list . [1, 2, 3, 4, 5, 6] . compre_double_list = [value * 2 for value in a_list] compre_double_list . [2, 4, 6, 8, 10, 12] . Breaking down by the syntax: . transformer_expression : value * 2 | iterator_varaible : value | sequence : a_list | . filterEvenList = list(filter(lambda elem: elem % 2 == 0 , mixList)) print(filterEvenList) . [2, 88, 22, 14, 0, 8, 10, 20, 4] . mixList . [2, 88, 33, 22, 14, 0, 8, 10, 20, 4] . filterListComprehension = [element for element in mixList if element % 2 == 0] filterListComprehension . [2, 88, 22, 14, 0, 8, 10, 20, 4] . filterListComprehension == filterEvenList . True . Write code to assign to the variable compri all the values of the key name in any of the sub-dictionaries in the dictionary tester. Do this using a list comprehension. . tester = {&#39;info&#39;: [{&quot;name&quot;: &quot;Lauren&quot;, &#39;class standing&#39;: &#39;Junior&#39;, &#39;major&#39;: &quot;Information Science&quot;},{&#39;name&#39;: &#39;Ayo&#39;, &#39;class standing&#39;: &quot;Bachelor&#39;s&quot;, &#39;major&#39;: &#39;Information Science&#39;}, {&#39;name&#39;: &#39;Kathryn&#39;, &#39;class standing&#39;: &#39;Senior&#39;, &#39;major&#39;: &#39;Sociology&#39;}, {&#39;name&#39;: &#39;Nick&#39;, &#39;class standing&#39;: &#39;Junior&#39;, &#39;major&#39;: &#39;Computer Science&#39;}, {&#39;name&#39;: &#39;Gladys&#39;, &#39;class standing&#39;: &#39;Sophomore&#39;, &#39;major&#39;: &#39;History&#39;}, {&#39;name&#39;: &#39;Adam&#39;, &#39;major&#39;: &#39;Violin Performance&#39;, &#39;class standing&#39;: &#39;Senior&#39;}]} inner_list = tester[&#39;info&#39;] #print(inner_list) # For Readability import json print(json.dumps(inner_list , indent = 2)) . [ { &#34;name&#34;: &#34;Lauren&#34;, &#34;class standing&#34;: &#34;Junior&#34;, &#34;major&#34;: &#34;Information Science&#34; }, { &#34;name&#34;: &#34;Ayo&#34;, &#34;class standing&#34;: &#34;Bachelor&#39;s&#34;, &#34;major&#34;: &#34;Information Science&#34; }, { &#34;name&#34;: &#34;Kathryn&#34;, &#34;class standing&#34;: &#34;Senior&#34;, &#34;major&#34;: &#34;Sociology&#34; }, { &#34;name&#34;: &#34;Nick&#34;, &#34;class standing&#34;: &#34;Junior&#34;, &#34;major&#34;: &#34;Computer Science&#34; }, { &#34;name&#34;: &#34;Gladys&#34;, &#34;class standing&#34;: &#34;Sophomore&#34;, &#34;major&#34;: &#34;History&#34; }, { &#34;name&#34;: &#34;Adam&#34;, &#34;major&#34;: &#34;Violin Performance&#34;, &#34;class standing&#34;: &#34;Senior&#34; } ] . nameList = [] if True: for dict_name in inner_list: name = dict_name[&#39;name&#39;] nameList.append(name) print(nameList) . [&#39;Lauren&#39;, &#39;Ayo&#39;, &#39;Kathryn&#39;, &#39;Nick&#39;, &#39;Gladys&#39;, &#39;Adam&#39;] . compri = [dict_value[&#39;name&#39;] for dict_value in inner_list if True] compri . [&#39;Lauren&#39;, &#39;Ayo&#39;, &#39;Kathryn&#39;, &#39;Nick&#39;, &#39;Gladys&#39;, &#39;Adam&#39;] .",
            "url": "https://ashikshafi08.github.io/fastpages/jupyter/2021/04/11/python-goodies.html",
            "relUrl": "/jupyter/2021/04/11/python-goodies.html",
            "date": " ‚Ä¢ Apr 11, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://ashikshafi08.github.io/fastpages/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://ashikshafi08.github.io/fastpages/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hey it‚Äôs Ashik here! I ma Machine Learning Practitioner. .",
          "url": "https://ashikshafi08.github.io/fastpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://ashikshafi08.github.io/fastpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}