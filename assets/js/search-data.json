{
  
    
        "post0": {
            "title": "Intro to Maps, Filters and List Comprehension in Python",
            "content": "How to use Maps, Filters and List Comprehension in Python. . This notebook will covers the tutorial of map,filter and list comprehension. It&#39;s me learning these python goodies and documenting aside, so someday I could look up and revise stuffs. . I am going through the course on Coursera called Python-3 Programming from University of Michigan. Gotta admit I learnt alot of Python from there and still doing. Earlier I used to take notes on Notion, but now I thought of getting my hands dirty by coding along and take notes in Colab. . Also anyone interested or wanna refresh their Python skills could even make use of it. . Maps . Python provides built-in functions map and filter, even a new syntax called list comprehension that lets you express a mapping/filtering operation. Most documentations and programmers use list comprehension and it seem&#39;s more like a pythonic way of writing code. . Map, and filter are commands that you would use in high-performance computing on big datasets. http://en.wikipedia.org/wiki/MapReduce . def doubleStuff(a_list): &#39;&#39;&#39; Returns a new list in which contains doubles of the elements in a list &#39;&#39;&#39; # Accum list new_list = [] # Looping through values and making the calculation for value in a_list: double_elem = 2 * value new_list.append(double_elem) return new_list # Using the above function a_list = [1 , 2, 3 , 4, 5 , 6] print(f&#39;List before the values were got double: {a_list}&#39;) double_list = doubleStuff(a_list) print(f&#39; nThe list after the values they got double the number: {double_list}&#39;) . List before the values were got double: [1, 2, 3, 4, 5, 6] The list after the values they got double the number: [2, 4, 6, 8, 10, 12] . We can write the above function with less than one line of code using the map function. . Map is a function which takes functions as the first input and sequence as an second input. map(function , sequence). Map just says apply the transforms (function) to every element in this sequence. . Map always expects a transformer function. . def double(value): return 2*value map_double_list = list(map(double , a_list)) print(f&#39;Using map function: {map_double_list}&#39;) . Using map function: [2, 4, 6, 8, 10, 12] . I earlier tried just map(double , a_list) which gave me just the map object. It turns out to be enclosing the map object by a list will gives us the list object. . But why this happens? Map function returns an iterator, it doesn&#39;t want to store the list in it&#39;s memory. It&#39;s still an iterator and we can grab the list we needed by enclosing the map object by a list which prevents pain to memory . . # Multiply 5 to every value map_lambda_list = list(map(lambda value: 5*value , a_list)) print(f&#39;Multiplying 5 to every element: {map_lambda_list}&#39;) . Multiplying 5 to every element: [5, 10, 15, 20, 25, 30] . Let&#39;s Solve some Problems! . Gotta go through more: https://www.w3resource.com/python-exercises/map/index.php . Below we have provided a list of strings called abbrevs. Use map to produce a new list called abbrevs_upper that contains all the same strings in upper case. | abbrevs = [&#39;usa&#39; , &#39;esp&#39; , &#39;chn&#39; , &#39;jpn&#39; , &#39;mex&#39; , &#39;can&#39; , &#39;rus&#39; , &#39;rsa&#39; , &#39;jam&#39;] . upperAbbrev_list = [] for abbrev in abbrevs: upperAbbrev_list.append(abbrev.upper()) print(upperAbbrev_list) . [&#39;USA&#39;, &#39;ESP&#39;, &#39;CHN&#39;, &#39;JPN&#39;, &#39;MEX&#39;, &#39;CAN&#39;, &#39;RUS&#39;, &#39;RSA&#39;, &#39;JAM&#39;] . upperCase_abbrevs = list(map(lambda abbrev: abbrev.upper() , abbrevs)) print(upperCase_abbrevs) . [&#39;USA&#39;, &#39;ESP&#39;, &#39;CHN&#39;, &#39;JPN&#39;, &#39;MEX&#39;, &#39;CAN&#39;, &#39;RUS&#39;, &#39;RSA&#39;, &#39;JAM&#39;] . Using map, create a list assigned to the variable greeting_doubled that doubles each element in the list. | lst = [[&quot;hi&quot;, &quot;bye&quot;], &quot;hello&quot;, &quot;goodbye&quot;, [9, 2], 4] . lst = [[&quot;hi&quot;, &quot;bye&quot;], &quot;hello&quot;, &quot;goodbye&quot;, [9, 2], 4] greeting_doubled = list(map(lambda element: 2 * element , lst)) print(greeting_doubled) . [[&#39;hi&#39;, &#39;bye&#39;, &#39;hi&#39;, &#39;bye&#39;], &#39;hellohello&#39;, &#39;goodbyegoodbye&#39;, [9, 2, 9, 2], 8] . Write a Python program to add three given lists using Python map and lambda | list(map(lambda a,b,c: a + b + c , [1 , 2 ,3] , [4 ,5 , 6] , [7 ,8 , 9])) . [12, 15, 18] . Filters . Filter function filter takes two arguments same like our map which has both function and a sequence parameters. Instead mapping them or making calculation with eachother, filter filters out the numbers either True or False. . The function takes one item and return True if the item should. It is automatically called for each item in the sequence . filter returns an iterator object like map, so we gotta wrap them by list. . def keep_evens(a_list): new_list = [] for elem in a_list: if elem % 2 == 0: new_list.append(elem) return new_list # Using the above function mixList = [2 , 88 , 33 , 22 , 14 , 0 , 8 , 10 , 20 , 4] evenList = keep_evens(mixList) print(evenList) . [2, 88, 22, 14, 0, 8, 10, 20, 4] . filterEvenList = list(filter(lambda elem: elem % 2 == 0 , mixList)) print(filterEvenList) . [2, 88, 22, 14, 0, 8, 10, 20, 4] . Let&#39;s Solve some Problems . Using filter, filter lst so that it only contains words containing the letter &#39;o&#39;. Assign to variable lst2. | lst = [&#39;witch&#39; , &#39;halloween&#39; , &#39;pumpkin&#39; , &#39;cat&#39; , &#39;candy&#39; , &#39;wagon&#39; ,&#39;moon&#39;] . lst = [&#39;witch&#39; , &#39;halloween&#39; , &#39;pumpkin&#39; , &#39;cat&#39; , &#39;candy&#39; , &#39;wagon&#39; , &#39;moon&#39;] lst2 = list(filter(lambda elem: &#39;o&#39; in elem , lst)) print(lst2) . [&#39;halloween&#39;, &#39;wagon&#39;, &#39;moon&#39;] . Write code to assign to the variable filter_testing all the elements in lst_check that have a &#39;w&#39; in them using filter. | lst_check = [&#39;plums&#39;, &#39;watermelon&#39;, &#39;kiwi&#39;, &#39;strawberries&#39;, &#39;blueberries&#39;, &#39;peaches&#39;, &#39;apples&#39;, &#39;mangos&#39;, &#39;papaya&#39;] . lst_check = [&#39;plums&#39;, &#39;watermelon&#39;, &#39;kiwi&#39;, &#39;strawberries&#39;, &#39;blueberries&#39;, &#39;peaches&#39;, &#39;apples&#39;, &#39;mangos&#39;, &#39;papaya&#39;] filter_testing = list(filter(lambda word: &#39;w&#39; in word , lst_check)) print(filter_testing) . [&#39;watermelon&#39;, &#39;kiwi&#39;, &#39;strawberries&#39;] . List Comprehensions . Before we saw those two functions namely map and filter turns out to be we don&#39;t want to use them much (or) in other words we can use list comprehensions inplace of using map and filter. Better we can pull off more flexibility by using list comprehensions. . In simple words list comprehensions is a convinient syntax to do map and filter operations. . Basic Syntax of list comprehension: . [ &lt;transformer_expression&gt; for &lt;iterator_variable&gt; in &lt;sequence&gt; if &lt;filteration_expression&gt;] . def double(value): return 2*value map_double_list = list(map(double , a_list)) print(f&#39;Using map function: {map_double_list}&#39;) . Using map function: [2, 4, 6, 8, 10, 12] . a_list . [1, 2, 3, 4, 5, 6] . compre_double_list = [value * 2 for value in a_list] compre_double_list . [2, 4, 6, 8, 10, 12] . Breaking down by the syntax: . transformer_expression : value * 2 | iterator_varaible : value | sequence : a_list | . filterEvenList = list(filter(lambda elem: elem % 2 == 0 , mixList)) print(filterEvenList) . [2, 88, 22, 14, 0, 8, 10, 20, 4] . mixList . [2, 88, 33, 22, 14, 0, 8, 10, 20, 4] . filterListComprehension = [element for element in mixList if element % 2 == 0] filterListComprehension . [2, 88, 22, 14, 0, 8, 10, 20, 4] . filterListComprehension == filterEvenList . True . Write code to assign to the variable compri all the values of the key name in any of the sub-dictionaries in the dictionary tester. Do this using a list comprehension. . tester = {&#39;info&#39;: [{&quot;name&quot;: &quot;Lauren&quot;, &#39;class standing&#39;: &#39;Junior&#39;, &#39;major&#39;: &quot;Information Science&quot;},{&#39;name&#39;: &#39;Ayo&#39;, &#39;class standing&#39;: &quot;Bachelor&#39;s&quot;, &#39;major&#39;: &#39;Information Science&#39;}, {&#39;name&#39;: &#39;Kathryn&#39;, &#39;class standing&#39;: &#39;Senior&#39;, &#39;major&#39;: &#39;Sociology&#39;}, {&#39;name&#39;: &#39;Nick&#39;, &#39;class standing&#39;: &#39;Junior&#39;, &#39;major&#39;: &#39;Computer Science&#39;}, {&#39;name&#39;: &#39;Gladys&#39;, &#39;class standing&#39;: &#39;Sophomore&#39;, &#39;major&#39;: &#39;History&#39;}, {&#39;name&#39;: &#39;Adam&#39;, &#39;major&#39;: &#39;Violin Performance&#39;, &#39;class standing&#39;: &#39;Senior&#39;}]} inner_list = tester[&#39;info&#39;] #print(inner_list) # For Readability import json print(json.dumps(inner_list , indent = 2)) . [ { &#34;name&#34;: &#34;Lauren&#34;, &#34;class standing&#34;: &#34;Junior&#34;, &#34;major&#34;: &#34;Information Science&#34; }, { &#34;name&#34;: &#34;Ayo&#34;, &#34;class standing&#34;: &#34;Bachelor&#39;s&#34;, &#34;major&#34;: &#34;Information Science&#34; }, { &#34;name&#34;: &#34;Kathryn&#34;, &#34;class standing&#34;: &#34;Senior&#34;, &#34;major&#34;: &#34;Sociology&#34; }, { &#34;name&#34;: &#34;Nick&#34;, &#34;class standing&#34;: &#34;Junior&#34;, &#34;major&#34;: &#34;Computer Science&#34; }, { &#34;name&#34;: &#34;Gladys&#34;, &#34;class standing&#34;: &#34;Sophomore&#34;, &#34;major&#34;: &#34;History&#34; }, { &#34;name&#34;: &#34;Adam&#34;, &#34;major&#34;: &#34;Violin Performance&#34;, &#34;class standing&#34;: &#34;Senior&#34; } ] . nameList = [] if True: for dict_name in inner_list: name = dict_name[&#39;name&#39;] nameList.append(name) print(nameList) . [&#39;Lauren&#39;, &#39;Ayo&#39;, &#39;Kathryn&#39;, &#39;Nick&#39;, &#39;Gladys&#39;, &#39;Adam&#39;] . compri = [dict_value[&#39;name&#39;] for dict_value in inner_list if True] compri . [&#39;Lauren&#39;, &#39;Ayo&#39;, &#39;Kathryn&#39;, &#39;Nick&#39;, &#39;Gladys&#39;, &#39;Adam&#39;] .",
            "url": "https://ashikshafi08.github.io/fastpages/jupyter/2021/04/11/python-goodies.html",
            "relUrl": "/jupyter/2021/04/11/python-goodies.html",
            "date": " • Apr 11, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Introduction to Convolutional Neural Networks and Computer Vision with TensorFlow Tutorial",
            "content": "So far we&#39;ve covered the basics of TensorFlow and built a handful of models to work across different problems. . Now we&#39;re going to get specific and see how a special kind of neural network, convolutional neural networks (CNNs) can be used for computer vision (detecting patterns in visual data). . 🔑 Note: In deep learning, many different kinds of model architectures can be used for different problems. For example, you could use a convolutional neural network for making predictions on image data and/or text data. However, in practice some architectures typically work better than others. For example, you might want to: . Classify whether a picture of food contains pizza 🍕 or steak 🥩 (we&#39;re actually going to do this) | Detect whether or not an object appears in an image (e.g. did a specific car pass through a security camera?) | . In this notebook, we&#39;re going to follow the TensorFlow modelling workflow we&#39;ve been following so far whilst learning about how to build and use CNNs. . What we&#39;re going to cover . Specifically, we&#39;re going to go through the follow with TensorFlow: . Getting a dataset to work with | Architecture of a convolutional neural network | A quick end-to-end example (what we&#39;re working towards) | Steps in modelling for binary image classification with CNNs Becoming one with the data | Preparing data for modelling | Creating a CNN model (starting with a baseline) | Fitting a model (getting it to find patterns in our data) | Evaluating a model | Improving a model | Making a prediction with a trained model | . | Steps in modelling for multi-class image classification with CNNs Same as above (but this time with a different dataset) | . | . How you can use this notebook . You can read through the descriptions and the code (it should all run, except for the cells which error on purpose), but there&#39;s a better option. . Write all of the code yourself. . Yes. I&#39;m serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break? . You don&#39;t have to write the text descriptions but writing the code yourself is a great way to get hands-on experience. . Don&#39;t worry if you make mistakes, we all do. The way to get better and make less mistakes is to write more code. . Get the data . Because convolutional neural networks work so well with images, to learn more about them, we&#39;re going to start with a dataset of images. . The images we&#39;re going to work with are from the Food-101 dataset, a collection of 101 different categories of 101,000 (1000 images per category) real-world images of food dishes. . To begin, we&#39;re only going to use two of the categories, pizza 🍕 and steak 🥩 and build a binary classifier. . 🔑 Note: To prepare the data we&#39;re using, preprocessing steps such as, moving the images into different subset folders, have been done. To see these preprocessing steps check out the preprocessing notebook. We&#39;ll download the pizza_steak subset .zip file and unzip it. . import zipfile # Download zip file of pizza_steak images !wget https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip # Unzip the downloaded file zip_ref = zipfile.ZipFile(&quot;pizza_steak.zip&quot;, &quot;r&quot;) zip_ref.extractall() zip_ref.close() . --2021-01-25 22:55:05-- https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.142.128, 74.125.195.128, 74.125.20.128, ... Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.142.128|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 109579078 (105M) [application/zip] Saving to: ‘pizza_steak.zip’ pizza_steak.zip 100%[===================&gt;] 104.50M 208MB/s in 0.5s 2021-01-25 22:55:06 (208 MB/s) - ‘pizza_steak.zip’ saved [109579078/109579078] . 🔑 Note: If you&#39;re using Google Colab and your runtime disconnects, you may have to redownload the files. You can do this by rerunning the cell above. . Inspect the data (become one with it) . A very crucial step at the beginning of any machine learning project is becoming one with the data. This usually means plenty of visualizing and folder scanning to understand the data you&#39;re working with. . Wtih this being said, let&#39;s inspect the data we just downloaded. . The file structure has been formatted to be in a typical format you might use for working with images. . More specifically: . A train directory which contains all of the images in the training dataset with subdirectories each named after a certain class containing images of that class. | A test directory with the same structure as the train directory. | . # Example of file structure pizza_steak &lt;- top level folder └───train &lt;- training images │ └───pizza │ │ │ 1008104.jpg │ │ │ 1638227.jpg │ │ │ ... │ └───steak │ │ 1000205.jpg │ │ 1647351.jpg │ │ ... │ └───test &lt;- testing images │ └───pizza │ │ │ 1001116.jpg │ │ │ 1507019.jpg │ │ │ ... │ └───steak │ │ 100274.jpg │ │ 1653815.jpg │ │ ... . Let&#39;s inspect each of the directories we&#39;ve downloaded. . To so do, we can use the command ls which stands for list. . !ls pizza_steak . test train . We can see we&#39;ve got a train and test folder. . Let&#39;s see what&#39;s inside one of them. . !ls pizza_steak/train/ . pizza steak . And how about insde the steak directory? . !ls pizza_steak/train/steak/ . 1000205.jpg 1647351.jpg 2238681.jpg 2824680.jpg 3375959.jpg 417368.jpg 100135.jpg 1650002.jpg 2238802.jpg 2825100.jpg 3381560.jpg 4176.jpg 101312.jpg 165639.jpg 2254705.jpg 2826987.jpg 3382936.jpg 42125.jpg 1021458.jpg 1658186.jpg 225990.jpg 2832499.jpg 3386119.jpg 421476.jpg 1032846.jpg 1658443.jpg 2260231.jpg 2832960.jpg 3388717.jpg 421561.jpg 10380.jpg 165964.jpg 2268692.jpg 285045.jpg 3389138.jpg 438871.jpg 1049459.jpg 167069.jpg 2271133.jpg 285147.jpg 3393547.jpg 43924.jpg 1053665.jpg 1675632.jpg 227576.jpg 2855315.jpg 3393688.jpg 440188.jpg 1068516.jpg 1678108.jpg 2283057.jpg 2856066.jpg 3396589.jpg 442757.jpg 1068975.jpg 168006.jpg 2286639.jpg 2859933.jpg 339891.jpg 443210.jpg 1081258.jpg 1682496.jpg 2287136.jpg 286219.jpg 3417789.jpg 444064.jpg 1090122.jpg 1684438.jpg 2291292.jpg 2862562.jpg 3425047.jpg 444709.jpg 1093966.jpg 168775.jpg 229323.jpg 2865730.jpg 3434983.jpg 447557.jpg 1098844.jpg 1697339.jpg 2300534.jpg 2878151.jpg 3435358.jpg 461187.jpg 1100074.jpg 1710569.jpg 2300845.jpg 2880035.jpg 3438319.jpg 461689.jpg 1105280.jpg 1714605.jpg 231296.jpg 2881783.jpg 3444407.jpg 465494.jpg 1117936.jpg 1724387.jpg 2315295.jpg 2884233.jpg 345734.jpg 468384.jpg 1126126.jpg 1724717.jpg 2323132.jpg 2890573.jpg 3460673.jpg 477486.jpg 114601.jpg 172936.jpg 2324994.jpg 2893832.jpg 3465327.jpg 482022.jpg 1147047.jpg 1736543.jpg 2327701.jpg 2893892.jpg 3466159.jpg 482465.jpg 1147883.jpg 1736968.jpg 2331076.jpg 2907177.jpg 3469024.jpg 483788.jpg 1155665.jpg 1746626.jpg 233964.jpg 290850.jpg 3470083.jpg 493029.jpg 1163977.jpg 1752330.jpg 2344227.jpg 2909031.jpg 3476564.jpg 503589.jpg 1190233.jpg 1761285.jpg 234626.jpg 2910418.jpg 3478318.jpg 510757.jpg 1208405.jpg 176508.jpg 234704.jpg 2912290.jpg 3488748.jpg 513129.jpg 1209120.jpg 1772039.jpg 2357281.jpg 2916448.jpg 3492328.jpg 513842.jpg 1212161.jpg 1777107.jpg 2361812.jpg 2916967.jpg 3518960.jpg 523535.jpg 1213988.jpg 1787505.jpg 2365287.jpg 2927833.jpg 3522209.jpg 525041.jpg 1219039.jpg 179293.jpg 2374582.jpg 2928643.jpg 3524429.jpg 534560.jpg 1225762.jpg 1816235.jpg 239025.jpg 2929179.jpg 3528458.jpg 534633.jpg 1230968.jpg 1822407.jpg 2390628.jpg 2936477.jpg 3531805.jpg 536535.jpg 1236155.jpg 1823263.jpg 2392910.jpg 2938012.jpg 3536023.jpg 541410.jpg 1241193.jpg 1826066.jpg 2394465.jpg 2938151.jpg 3538682.jpg 543691.jpg 1248337.jpg 1828502.jpg 2395127.jpg 2939678.jpg 3540750.jpg 560503.jpg 1257104.jpg 1828969.jpg 2396291.jpg 2940544.jpg 354329.jpg 561972.jpg 126345.jpg 1829045.jpg 2400975.jpg 2940621.jpg 3547166.jpg 56240.jpg 1264050.jpg 1829088.jpg 2403776.jpg 2949079.jpg 3553911.jpg 56409.jpg 1264154.jpg 1836332.jpg 2403907.jpg 295491.jpg 3556871.jpg 564530.jpg 1264858.jpg 1839025.jpg 240435.jpg 296268.jpg 355715.jpg 568972.jpg 127029.jpg 1839481.jpg 2404695.jpg 2964732.jpg 356234.jpg 576725.jpg 1289900.jpg 183995.jpg 2404884.jpg 2965021.jpg 3571963.jpg 588739.jpg 1290362.jpg 184110.jpg 2407770.jpg 2966859.jpg 3576078.jpg 590142.jpg 1295457.jpg 184226.jpg 2412263.jpg 2977966.jpg 3577618.jpg 60633.jpg 1312841.jpg 1846706.jpg 2425062.jpg 2979061.jpg 3577732.jpg 60655.jpg 1313316.jpg 1849364.jpg 2425389.jpg 2983260.jpg 3578934.jpg 606820.jpg 1324791.jpg 1849463.jpg 2435316.jpg 2984311.jpg 358042.jpg 612551.jpg 1327567.jpg 1849542.jpg 2437268.jpg 2988960.jpg 358045.jpg 614975.jpg 1327667.jpg 1853564.jpg 2437843.jpg 2989882.jpg 3591821.jpg 616809.jpg 1333055.jpg 1869467.jpg 2440131.jpg 2995169.jpg 359330.jpg 628628.jpg 1334054.jpg 1870942.jpg 2443168.jpg 2996324.jpg 3601483.jpg 632427.jpg 1335556.jpg 187303.jpg 2446660.jpg 3000131.jpg 3606642.jpg 636594.jpg 1337814.jpg 187521.jpg 2455944.jpg 3002350.jpg 3609394.jpg 637374.jpg 1340977.jpg 1888450.jpg 2458401.jpg 3007772.jpg 361067.jpg 640539.jpg 1343209.jpg 1889336.jpg 2487306.jpg 3008192.jpg 3613455.jpg 644777.jpg 134369.jpg 1907039.jpg 248841.jpg 3009617.jpg 3621464.jpg 644867.jpg 1344105.jpg 1925230.jpg 2489716.jpg 3011642.jpg 3621562.jpg 658189.jpg 134598.jpg 1927984.jpg 2490489.jpg 3020591.jpg 3621565.jpg 660900.jpg 1346387.jpg 1930577.jpg 2495884.jpg 3030578.jpg 3623556.jpg 663014.jpg 1348047.jpg 1937872.jpg 2495903.jpg 3047807.jpg 3640915.jpg 664545.jpg 1351372.jpg 1941807.jpg 2499364.jpg 3059843.jpg 3643951.jpg 667075.jpg 1362989.jpg 1942333.jpg 2500292.jpg 3074367.jpg 3653129.jpg 669180.jpg 1367035.jpg 1945132.jpg 2509017.jpg 3082120.jpg 3656752.jpg 669960.jpg 1371177.jpg 1961025.jpg 250978.jpg 3094354.jpg 3663518.jpg 6709.jpg 1375640.jpg 1966300.jpg 2514432.jpg 3095301.jpg 3663800.jpg 674001.jpg 1382427.jpg 1966967.jpg 2526838.jpg 3099645.jpg 3664376.jpg 676189.jpg 1392718.jpg 1969596.jpg 252858.jpg 3100476.jpg 3670607.jpg 681609.jpg 1395906.jpg 1971757.jpg 2532239.jpg 3110387.jpg 3671021.jpg 6926.jpg 1400760.jpg 1976160.jpg 2534567.jpg 3113772.jpg 3671877.jpg 703556.jpg 1403005.jpg 1984271.jpg 2535431.jpg 3116018.jpg 368073.jpg 703909.jpg 1404770.jpg 1987213.jpg 2535456.jpg 3128952.jpg 368162.jpg 704316.jpg 140832.jpg 1987639.jpg 2538000.jpg 3130412.jpg 368170.jpg 714298.jpg 141056.jpg 1995118.jpg 2543081.jpg 3136.jpg 3693649.jpg 720060.jpg 141135.jpg 1995252.jpg 2544643.jpg 313851.jpg 3700079.jpg 726083.jpg 1413972.jpg 199754.jpg 2547797.jpg 3140083.jpg 3704103.jpg 728020.jpg 1421393.jpg 2002400.jpg 2548974.jpg 3140147.jpg 3707493.jpg 732986.jpg 1428947.jpg 2011264.jpg 2549316.jpg 3142045.jpg 3716881.jpg 734445.jpg 1433912.jpg 2012996.jpg 2561199.jpg 3142618.jpg 3724677.jpg 735441.jpg 143490.jpg 2013535.jpg 2563233.jpg 3142674.jpg 3727036.jpg 740090.jpg 1445352.jpg 2017387.jpg 256592.jpg 3143192.jpg 3727491.jpg 745189.jpg 1446401.jpg 2018173.jpg 2568848.jpg 314359.jpg 3736065.jpg 752203.jpg 1453991.jpg 2020613.jpg 2573392.jpg 3157832.jpg 37384.jpg 75537.jpg 1456841.jpg 2032669.jpg 2592401.jpg 3159818.jpg 3743286.jpg 756655.jpg 146833.jpg 203450.jpg 2599817.jpg 3162376.jpg 3745515.jpg 762210.jpg 1476404.jpg 2034628.jpg 2603058.jpg 3168620.jpg 3750472.jpg 763690.jpg 1485083.jpg 2036920.jpg 2606444.jpg 3171085.jpg 3752362.jpg 767442.jpg 1487113.jpg 2038418.jpg 2614189.jpg 317206.jpg 3766099.jpg 786409.jpg 148916.jpg 2042975.jpg 2614649.jpg 3173444.jpg 3770370.jpg 80215.jpg 149087.jpg 2045647.jpg 2615718.jpg 3180182.jpg 377190.jpg 802348.jpg 1493169.jpg 2050584.jpg 2619625.jpg 31881.jpg 3777020.jpg 804684.jpg 149682.jpg 2052542.jpg 2622140.jpg 3191589.jpg 3777482.jpg 812163.jpg 1508094.jpg 2056627.jpg 262321.jpg 3204977.jpg 3781152.jpg 813486.jpg 1512226.jpg 2062248.jpg 2625330.jpg 320658.jpg 3787809.jpg 819027.jpg 1512347.jpg 2081995.jpg 2628106.jpg 3209173.jpg 3788729.jpg 822550.jpg 1524526.jpg 2087958.jpg 2629750.jpg 3223400.jpg 3790962.jpg 823766.jpg 1530833.jpg 2088030.jpg 2643906.jpg 3223601.jpg 3792514.jpg 827764.jpg 1539499.jpg 2088195.jpg 2644457.jpg 3241894.jpg 379737.jpg 830007.jpg 1541672.jpg 2090493.jpg 2648423.jpg 3245533.jpg 3807440.jpg 838344.jpg 1548239.jpg 2090504.jpg 2651300.jpg 3245622.jpg 381162.jpg 853327.jpg 1550997.jpg 2125877.jpg 2653594.jpg 3247009.jpg 3812039.jpg 854150.jpg 1552530.jpg 2129685.jpg 2661577.jpg 3253588.jpg 3829392.jpg 864997.jpg 15580.jpg 2133717.jpg 2668916.jpg 3260624.jpg 3830872.jpg 885571.jpg 1559052.jpg 2136662.jpg 268444.jpg 326587.jpg 38442.jpg 907107.jpg 1563266.jpg 213765.jpg 2691461.jpg 32693.jpg 3855584.jpg 908261.jpg 1567554.jpg 2138335.jpg 2706403.jpg 3271253.jpg 3857508.jpg 910672.jpg 1575322.jpg 2140776.jpg 270687.jpg 3274423.jpg 386335.jpg 911803.jpg 1588879.jpg 214320.jpg 2707522.jpg 3280453.jpg 3867460.jpg 91432.jpg 1594719.jpg 2146963.jpg 2711806.jpg 3298495.jpg 3868959.jpg 914570.jpg 1595869.jpg 215222.jpg 2716993.jpg 330182.jpg 3869679.jpg 922752.jpg 1598345.jpg 2154126.jpg 2724554.jpg 3306627.jpg 388776.jpg 923772.jpg 1598885.jpg 2154779.jpg 2738227.jpg 3315727.jpg 3890465.jpg 926414.jpg 1600179.jpg 2159975.jpg 2748917.jpg 331860.jpg 3894222.jpg 931356.jpg 1600794.jpg 2163079.jpg 2760475.jpg 332232.jpg 3895825.jpg 937133.jpg 160552.jpg 217250.jpg 2761427.jpg 3322909.jpg 389739.jpg 945791.jpg 1606596.jpg 2172600.jpg 2765887.jpg 332557.jpg 3916407.jpg 947877.jpg 1615395.jpg 2173084.jpg 2768451.jpg 3326734.jpg 393349.jpg 952407.jpg 1618011.jpg 217996.jpg 2771149.jpg 3330642.jpg 393494.jpg 952437.jpg 1619357.jpg 2193684.jpg 2779040.jpg 3333128.jpg 398288.jpg 955466.jpg 1621763.jpg 220341.jpg 2788312.jpg 3333735.jpg 40094.jpg 9555.jpg 1623325.jpg 22080.jpg 2788759.jpg 3334973.jpg 401094.jpg 961341.jpg 1624450.jpg 2216146.jpg 2796102.jpg 3335013.jpg 401144.jpg 97656.jpg 1624747.jpg 2222018.jpg 280284.jpg 3335267.jpg 401651.jpg 979110.jpg 1628861.jpg 2223787.jpg 2807888.jpg 3346787.jpg 405173.jpg 980247.jpg 1632774.jpg 2230959.jpg 2815172.jpg 3364420.jpg 405794.jpg 982988.jpg 1636831.jpg 2232310.jpg 2818805.jpg 336637.jpg 40762.jpg 987732.jpg 1645470.jpg 2233395.jpg 2823872.jpg 3372616.jpg 413325.jpg 996684.jpg . Woah, a whole bunch of images. But how many? . 🛠 Practice: Try listing the same information for the pizza directory in the test folder. . import os # Walk through pizza_steak directory and list number of files for dirpath, dirnames, filenames in os.walk(&quot;pizza_steak&quot;): print(f&quot;There are {len(dirnames)} directories and {len(filenames)} images in &#39;{dirpath}&#39;.&quot;) . There are 2 directories and 0 images in &#39;pizza_steak&#39;. There are 2 directories and 0 images in &#39;pizza_steak/train&#39;. There are 0 directories and 750 images in &#39;pizza_steak/train/steak&#39;. There are 0 directories and 750 images in &#39;pizza_steak/train/pizza&#39;. There are 2 directories and 0 images in &#39;pizza_steak/test&#39;. There are 0 directories and 250 images in &#39;pizza_steak/test/steak&#39;. There are 0 directories and 250 images in &#39;pizza_steak/test/pizza&#39;. . num_steak_images_train = len(os.listdir(&quot;pizza_steak/train/steak&quot;)) num_steak_images_train . 750 . import pathlib import numpy as np data_dir = pathlib.Path(&quot;pizza_steak/train/&quot;) # turn our training path into a Python path class_names = np.array(sorted([item.name for item in data_dir.glob(&#39;*&#39;)])) # created a list of class_names from the subdirectories print(class_names) . [&#39;pizza&#39; &#39;steak&#39;] . Okay, so we&#39;ve got a collection of 750 training images and 250 testing images of pizza and steak. . Let&#39;s look at some. . 🤔 Note: Whenever you&#39;re working with data, it&#39;s always good to visualize it as much as possible. Treat your first couple of steps of a project as becoming one with the data. Visualize, visualize, visualize. . import matplotlib.pyplot as plt import matplotlib.image as mpimg import random def view_random_image(target_dir, target_class): # Setup target directory (we&#39;ll view images from here) target_folder = target_dir+target_class # Get a random image path random_image = random.sample(os.listdir(target_folder), 1) # Read in the image and plot it using matplotlib img = mpimg.imread(target_folder + &quot;/&quot; + random_image[0]) plt.imshow(img) plt.title(target_class) plt.axis(&quot;off&quot;); print(f&quot;Image shape: {img.shape}&quot;) # show the shape of the image return img . img = view_random_image(target_dir=&quot;pizza_steak/train/&quot;, target_class=&quot;steak&quot;) . Image shape: (512, 512, 3) . After going through a dozen or so images from the different classes, you can start to get an idea of what we&#39;re working with. . The entire Food101 dataset comprises of similar images from 101 different classes. . You might&#39;ve noticed we&#39;ve been printing the image shape alongside the plotted image. . This is because the way our computer sees the image is in the form of a big array (tensor). . img . array([[[187, 201, 214], [146, 162, 175], [119, 135, 148], ..., [255, 145, 97], [249, 137, 89], [247, 132, 85]], [[137, 153, 166], [107, 123, 136], [ 89, 105, 120], ..., [254, 144, 95], [250, 140, 91], [250, 138, 90]], [[ 87, 105, 119], [ 70, 88, 102], [ 66, 83, 99], ..., [247, 141, 91], [248, 141, 89], [252, 145, 93]], ..., [[ 42, 65, 97], [ 44, 67, 99], [ 44, 67, 99], ..., [ 63, 76, 111], [ 62, 75, 110], [ 70, 83, 118]], [[ 48, 71, 105], [ 45, 68, 102], [ 44, 67, 99], ..., [ 63, 76, 111], [ 60, 73, 108], [ 67, 80, 115]], [[ 56, 79, 113], [ 51, 74, 108], [ 48, 71, 105], ..., [ 69, 79, 115], [ 62, 72, 108], [ 66, 76, 112]]], dtype=uint8) . img.shape # returns (width, height, colour channels) . (512, 512, 3) . Looking at the image shape more closely, you&#39;ll see it&#39;s in the form (Width, Height, Colour Channels). . In our case, the width and height vary but because we&#39;re dealing with colour images, the colour channels value is always 3. This is for different values of red, green and blue (RGB) pixels. . You&#39;ll notice all of the values in the img array are between 0 and 255. This is because that&#39;s the possible range for red, green and blue values. . For example, a pixel with a value red=0, green=0, blue=255 will look very blue. . So when we build a model to differentiate between our images of pizza and steak, it will be finding patterns in these different pixel values which determine what each class looks like. . 🔑 Note: As we&#39;ve discussed before, many machine learning models, including neural networks prefer the values they work with to be between 0 and 1. Knowing this, one of the most common preprocessing steps for working with images is to scale (also referred to as normalize) their pixel values by dividing the image arrays by 255. . img/255. . array([[[0.73333333, 0.78823529, 0.83921569], [0.57254902, 0.63529412, 0.68627451], [0.46666667, 0.52941176, 0.58039216], ..., [1. , 0.56862745, 0.38039216], [0.97647059, 0.5372549 , 0.34901961], [0.96862745, 0.51764706, 0.33333333]], [[0.5372549 , 0.6 , 0.65098039], [0.41960784, 0.48235294, 0.53333333], [0.34901961, 0.41176471, 0.47058824], ..., [0.99607843, 0.56470588, 0.37254902], [0.98039216, 0.54901961, 0.35686275], [0.98039216, 0.54117647, 0.35294118]], [[0.34117647, 0.41176471, 0.46666667], [0.2745098 , 0.34509804, 0.4 ], [0.25882353, 0.3254902 , 0.38823529], ..., [0.96862745, 0.55294118, 0.35686275], [0.97254902, 0.55294118, 0.34901961], [0.98823529, 0.56862745, 0.36470588]], ..., [[0.16470588, 0.25490196, 0.38039216], [0.17254902, 0.2627451 , 0.38823529], [0.17254902, 0.2627451 , 0.38823529], ..., [0.24705882, 0.29803922, 0.43529412], [0.24313725, 0.29411765, 0.43137255], [0.2745098 , 0.3254902 , 0.4627451 ]], [[0.18823529, 0.27843137, 0.41176471], [0.17647059, 0.26666667, 0.4 ], [0.17254902, 0.2627451 , 0.38823529], ..., [0.24705882, 0.29803922, 0.43529412], [0.23529412, 0.28627451, 0.42352941], [0.2627451 , 0.31372549, 0.45098039]], [[0.21960784, 0.30980392, 0.44313725], [0.2 , 0.29019608, 0.42352941], [0.18823529, 0.27843137, 0.41176471], ..., [0.27058824, 0.30980392, 0.45098039], [0.24313725, 0.28235294, 0.42352941], [0.25882353, 0.29803922, 0.43921569]]]) . A (typical) architecture of a convolutional neural network . Convolutional neural networks are no different to other kinds of deep learning neural networks in the fact they can be created in many different ways. What you see below are some components you&#39;d expect to find in a traditional CNN. . Components of a convolutional neural network: . Hyperparameter/Layer type What does it do? Typical values . Input image(s) | Target images you&#39;d like to discover patterns in | Whatever you can take a photo (or video) of | . Input layer | Takes in target images and preprocesses them for further layers | input_shape = [batch_size, image_height, image_width, color_channels] | . Convolution layer | Extracts/learns the most important features from target images | Multiple, can create with tf.keras.layers.ConvXD (X can be multiple values) | . Hidden activation | Adds non-linearity to learned features (non-straight lines) | Usually ReLU (tf.keras.activations.relu) | . Pooling layer | Reduces the dimensionality of learned image features | Average (tf.keras.layers.AvgPool2D) or Max (tf.keras.layers.MaxPool2D) | . Fully connected layer | Further refines learned features from convolution layers | tf.keras.layers.Dense | . Output layer | Takes learned features and outputs them in shape of target labels | output_shape = [number_of_classes] (e.g. 3 for pizza, steak or sushi) | . Output activation | Adds non-linearities to output layer | tf.keras.activations.sigmoid (binary classification) or tf.keras.activations.softmax | . How they stack together: . A simple example of how you might stack together the above layers into a convolutional neural network. Note the convolutional and pooling layers can often be arranged and rearranged into many different formations. . An end-to-end example . We&#39;ve checked out our data and found there&#39;s 750 training images, as well as 250 test images per class and they&#39;re all of various different shapes. . It&#39;s time to jump straight in the deep end. . Reading the original dataset authors paper, we see they used a Random Forest machine learning model and averaged 50.76% accuracy at predicting what different foods different images had in them. . From now on, that 50.76% will be our baseline. . 🔑 Note: A baseline is a score or evaluation metric you want to try and beat. Usually you&#39;ll start with a simple model, create a baseline and try to beat it by increasing the complexity of the model. A really fun way to learn machine learning is to find some kind of modelling paper with a published result and try to beat it. The code in the following cell replicates and end-to-end way to model our pizza_steak dataset with a convolutional neural network (CNN) using the components listed above. . There will be a bunch of things you might not recognize but step through the code yourself and see if you can figure out what it&#39;s doing. . We&#39;ll go through each of the steps later on in the notebook. . For reference, the model we&#39;re using replicates TinyVGG, the computer vision architecture which fuels the CNN explainer webpage. . 📖 Resource: The architecture we&#39;re using below is a scaled-down version of VGG-16, a convolutional neural network which came 2nd in the 2014 ImageNet classification competition. . import tensorflow as tf from tensorflow.keras.preprocessing.image import ImageDataGenerator # Set the seed tf.random.set_seed(42) # Preprocess data (get all of the pixel values between 1 and 0, also called scaling/normalization) train_datagen = ImageDataGenerator(rescale=1./255) valid_datagen = ImageDataGenerator(rescale=1./255) # Setup the train and test directories train_dir = &quot;pizza_steak/train/&quot; test_dir = &quot;pizza_steak/test/&quot; # Import data from directories and turn it into batches train_data = train_datagen.flow_from_directory(train_dir, batch_size=32, # number of images to process at a time target_size=(224, 224), # convert all images to be 224 x 224 class_mode=&quot;binary&quot;, # type of problem we&#39;re working on seed=42) valid_data = valid_datagen.flow_from_directory(test_dir, batch_size=32, target_size=(224, 224), class_mode=&quot;binary&quot;, seed=42) # Create a CNN model (same as Tiny VGG - https://poloclub.github.io/cnn-explainer/) model_1 = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(filters=10, kernel_size=3, # can also be (3, 3) activation=&quot;relu&quot;, input_shape=(224, 224, 3)), # first layer specifies input shape (height, width, colour channels) tf.keras.layers.Conv2D(10, 3, activation=&quot;relu&quot;), tf.keras.layers.MaxPool2D(pool_size=2, # pool_size can also be (2, 2) padding=&quot;valid&quot;), # padding can also be &#39;same&#39; tf.keras.layers.Conv2D(10, 3, activation=&quot;relu&quot;), tf.keras.layers.Conv2D(10, 3, activation=&quot;relu&quot;), # activation=&#39;relu&#39; == tf.keras.layers.Activations(tf.nn.relu) tf.keras.layers.MaxPool2D(2), tf.keras.layers.Flatten(), tf.keras.layers.Dense(1, activation=&quot;sigmoid&quot;) # binary activation output ]) # Compile the model model_1.compile(loss=&quot;binary_crossentropy&quot;, optimizer=tf.keras.optimizers.Adam(), metrics=[&quot;accuracy&quot;]) # Fit the model history_1 = model_1.fit(train_data, epochs=5, steps_per_epoch=len(train_data), validation_data=valid_data, validation_steps=len(valid_data)) . Found 1500 images belonging to 2 classes. Found 500 images belonging to 2 classes. Epoch 1/5 47/47 [==============================] - 18s 219ms/step - loss: 0.6561 - accuracy: 0.5468 - val_loss: 0.4005 - val_accuracy: 0.8340 Epoch 2/5 47/47 [==============================] - 10s 208ms/step - loss: 0.4437 - accuracy: 0.7897 - val_loss: 0.4072 - val_accuracy: 0.8300 Epoch 3/5 47/47 [==============================] - 10s 207ms/step - loss: 0.4003 - accuracy: 0.8127 - val_loss: 0.3743 - val_accuracy: 0.8540 Epoch 4/5 47/47 [==============================] - 10s 205ms/step - loss: 0.3843 - accuracy: 0.8381 - val_loss: 0.3254 - val_accuracy: 0.8620 Epoch 5/5 47/47 [==============================] - 10s 205ms/step - loss: 0.3526 - accuracy: 0.8498 - val_loss: 0.3303 - val_accuracy: 0.8580 . 🤔 Note: If the cell above takes more than ~12 seconds per epoch to run, you might not be using a GPU accelerator. If you&#39;re using a Colab notebook, you can access a GPU accelerator by going to Runtime -&gt; Change Runtime Type -&gt; Hardware Accelerator and select &quot;GPU&quot;. After doing so, you might have to rerun all of the above cells as changing the runtime type causes Colab to have to reset. Nice! After 5 epochs, our model beat the baseline score of 50.76% accuracy (our model got ~85% accuaracy on the training set and ~85% accuracy on the test set). . However, our model only went through a binary classificaiton problem rather than all of the 101 classes in the Food101 dataset, so we can&#39;t directly compare these metrics. That being said, the results so far show that our model is learning something. . 🛠 Practice: Step through each of the main blocks of code in the cell above, what do you think each is doing? It&#39;s okay if you&#39;re not sure, we&#39;ll go through this soon. In the meantime, spend 10-minutes playing around the incredible CNN explainer website. What do you notice about the layer names at the top of the webpage? . Since we&#39;ve already fit a model, let&#39;s check out its architecture. . model_1.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 222, 222, 10) 280 _________________________________________________________________ conv2d_1 (Conv2D) (None, 220, 220, 10) 910 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 110, 110, 10) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 108, 108, 10) 910 _________________________________________________________________ conv2d_3 (Conv2D) (None, 106, 106, 10) 910 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 53, 53, 10) 0 _________________________________________________________________ flatten (Flatten) (None, 28090) 0 _________________________________________________________________ dense (Dense) (None, 1) 28091 ================================================================= Total params: 31,101 Trainable params: 31,101 Non-trainable params: 0 _________________________________________________________________ . What do you notice about the names of model_1&#39;s layers and the layer names at the top of the CNN explainer website? . I&#39;ll let you in on a little secret: we&#39;ve replicated the exact architecture they use for their model demo. . Look at you go! You&#39;re already starting to replicate models you find in the wild. . Now there are a few new things here we haven&#39;t discussed, namely: . The ImageDataGenerator class and the rescale parameter | The flow_from_directory() method The batch_size parameter | The target_size parameter | . | Conv2D layers (and the parameters which come with them) | MaxPool2D layers (and their parameters). | The steps_per_epoch and validation_steps parameters in the fit() function | . Before we dive into each of these, let&#39;s see what happens if we try to fit a model we&#39;ve worked with previously to our data. . Using the same model as before . To examplify how neural networks can be adapted to many different problems, let&#39;s see how a binary classification model we&#39;ve previously built might work with our data. . 🔑 Note: If you haven&#39;t gone through the previous classification notebook, no troubles, we&#39;ll be bringing in the a simple 4 layer architecture used to separate dots replicated from the TensorFlow Playground environment. We can use all of the same parameters in our previous model except for changing two things: . The data - we&#39;re now working with images instead of dots. | The input shape - we have to tell our neural network the shape of the images we&#39;re working with. A common practice is to reshape images all to one size. In our case, we&#39;ll resize the images to (224, 224, 3), meaning a height and width of 224 pixels and a depth of 3 for the red, green, blue colour channels. | . | . tf.random.set_seed(42) # Create a model to replicate the TensorFlow Playground model model_2 = tf.keras.Sequential([ tf.keras.layers.Flatten(input_shape=(224, 224, 3)), # dense layers expect a 1-dimensional vector as input tf.keras.layers.Dense(4, activation=&#39;relu&#39;), tf.keras.layers.Dense(4, activation=&#39;relu&#39;), tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;) ]) # Compile the model model_2.compile(loss=&#39;binary_crossentropy&#39;, optimizer=tf.keras.optimizers.Adam(), metrics=[&quot;accuracy&quot;]) # Fit the model history_2 = model_2.fit(train_data, # use same training data created above epochs=5, steps_per_epoch=len(train_data), validation_data=valid_data, # use same validation data created above validation_steps=len(valid_data)) . Epoch 1/5 47/47 [==============================] - 9s 180ms/step - loss: 1.6346 - accuracy: 0.4876 - val_loss: 0.6932 - val_accuracy: 0.5000 Epoch 2/5 47/47 [==============================] - 8s 175ms/step - loss: 0.6932 - accuracy: 0.4976 - val_loss: 0.6932 - val_accuracy: 0.5000 Epoch 3/5 47/47 [==============================] - 8s 174ms/step - loss: 0.6934 - accuracy: 0.4808 - val_loss: 0.6932 - val_accuracy: 0.5000 Epoch 4/5 47/47 [==============================] - 8s 172ms/step - loss: 0.6931 - accuracy: 0.5093 - val_loss: 0.6932 - val_accuracy: 0.5000 Epoch 5/5 47/47 [==============================] - 8s 172ms/step - loss: 0.6933 - accuracy: 0.4898 - val_loss: 0.6932 - val_accuracy: 0.5000 . Hmmm... our model ran but it doesn&#39;t seem like it learned anything. It only reaches 50% accuracy on the training and test sets which in a binary classification problem is as good as guessing. . Let&#39;s see the architecture. . model_2.summary() . Model: &#34;sequential_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_1 (Flatten) (None, 150528) 0 _________________________________________________________________ dense_1 (Dense) (None, 4) 602116 _________________________________________________________________ dense_2 (Dense) (None, 4) 20 _________________________________________________________________ dense_3 (Dense) (None, 1) 5 ================================================================= Total params: 602,141 Trainable params: 602,141 Non-trainable params: 0 _________________________________________________________________ . Wow. One of the most noticeable things here is the much larger number of parameters in model_2 versus model_1. . model_2 has 602,141 trainable parameters where as model_1 has only 31,101. And despite this difference, model_1 still far and large out performs model_2. . 🔑 Note: You can think of trainable parameters as patterns a model can learn from data. Intuitiely, you might think more is better. And in some cases it is. But in this case, the difference here is in the two different styles of model we&#39;re using. Where a series of dense layers have a number of different learnable parameters connected to each other and hence a higher number of possible learnable patterns, a convolutional neural network seeks to sort out and learn the most important patterns in an image. So even though there are less learnable parameters in our convolutional neural network, these are often more helpful in decphering between different features in an image. Since our previous model didn&#39;t work, do you have any ideas of how we might make it work? . How about we increase the number of layers? . And maybe even increase the number of neurons in each layer? . More specifically, we&#39;ll increase the number of neurons (also called hidden units) in each dense layer from 4 to 100 and add an extra layer. . 🔑 Note: Adding extra layers or increasing the number of neurons in each layer is often referred to as increasing the complexity of your model. . tf.random.set_seed(42) # Create a model similar to model_1 but add an extra layer and increase the number of hidden units in each layer model_3 = tf.keras.Sequential([ tf.keras.layers.Flatten(input_shape=(224, 224, 3)), # dense layers expect a 1-dimensional vector as input tf.keras.layers.Dense(100, activation=&#39;relu&#39;), # increase number of neurons from 4 to 100 (for each layer) tf.keras.layers.Dense(100, activation=&#39;relu&#39;), tf.keras.layers.Dense(100, activation=&#39;relu&#39;), # add an extra layer tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;) ]) # Compile the model model_3.compile(loss=&#39;binary_crossentropy&#39;, optimizer=tf.keras.optimizers.Adam(), metrics=[&quot;accuracy&quot;]) # Fit the model history_3 = model_3.fit(train_data, epochs=5, steps_per_epoch=len(train_data), validation_data=valid_data, validation_steps=len(valid_data)) . Epoch 1/5 47/47 [==============================] - 9s 183ms/step - loss: 4.5677 - accuracy: 0.5918 - val_loss: 0.9606 - val_accuracy: 0.7200 Epoch 2/5 47/47 [==============================] - 8s 178ms/step - loss: 1.7679 - accuracy: 0.6584 - val_loss: 0.9279 - val_accuracy: 0.6660 Epoch 3/5 47/47 [==============================] - 8s 177ms/step - loss: 0.8468 - accuracy: 0.6778 - val_loss: 0.8536 - val_accuracy: 0.6820 Epoch 4/5 47/47 [==============================] - 8s 177ms/step - loss: 0.7119 - accuracy: 0.7563 - val_loss: 0.5009 - val_accuracy: 0.7580 Epoch 5/5 47/47 [==============================] - 8s 178ms/step - loss: 0.4894 - accuracy: 0.7908 - val_loss: 0.4503 - val_accuracy: 0.7940 . Woah! Looks like our model is learning again. It got ~70% accuracy on the training set and ~70% accuracy on the validation set. . How does the architecute look? . model_3.summary() . Model: &#34;sequential_2&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_2 (Flatten) (None, 150528) 0 _________________________________________________________________ dense_4 (Dense) (None, 100) 15052900 _________________________________________________________________ dense_5 (Dense) (None, 100) 10100 _________________________________________________________________ dense_6 (Dense) (None, 100) 10100 _________________________________________________________________ dense_7 (Dense) (None, 1) 101 ================================================================= Total params: 15,073,201 Trainable params: 15,073,201 Non-trainable params: 0 _________________________________________________________________ . My gosh, the number of trainable parameters has increased even more than model_2. And even with close to 500x (~15,000,000 vs. ~31,000) more trainable parameters, model_3 still doesn&#39;t out perform model_1. . This goes to show the power of convolutional neural networks and their ability to learn patterns despite using less parameters. . Binary classification: Let&#39;s break it down . We just went through a whirlwind of steps: . Become one with the data (visualize, visualize, visualize...) | Preprocess the data (prepare it for a model) | Create a model (start with a baseline) | Fit the model | Evaluate the model | Adjust different parameters and improve model (try to beat your baseline) | Repeat until satisfied | Let&#39;s step through each. . 1. Import and become one with the data . Whatever kind of data you&#39;re dealing with, it&#39;s a good idea to visualize at least 10-100 samples to start to building your own mental model of the data. . In our case, we might notice that the steak images tend to have darker colours where as pizza images tend to have a distinct circular shape in the middle. These might be patterns that our neural network picks up on. . You an also notice if some of your data is messed up (for example, has the wrong label) and start to consider ways you might go about fixing it. . 📖 Resource: To see how this data was processed into the file format we&#39;re using, see the preprocessing notebook. If the visualization cell below doesn&#39;t work, make sure you&#39;ve got the data by uncommenting the cell below. . # # Download zip file of pizza_steak images # !wget https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip # # Unzip the downloaded file # zip_ref = zipfile.ZipFile(&quot;pizza_steak.zip&quot;, &quot;r&quot;) # zip_ref.extractall() # zip_ref.close() . plt.figure() plt.subplot(1, 2, 1) steak_img = view_random_image(&quot;pizza_steak/train/&quot;, &quot;steak&quot;) plt.subplot(1, 2, 2) pizza_img = view_random_image(&quot;pizza_steak/train/&quot;, &quot;pizza&quot;) . Image shape: (384, 512, 3) Image shape: (512, 384, 3) . 2. Preprocess the data (prepare it for a model) . One of the most important steps for a machine learning project is creating a training and test set. . In our case, our data is already split into training and test sets. Another option here might be to create a validation set as well, but we&#39;ll leave that for now. . For an image classification project, it&#39;s standard to have your data seperated into train and test directories with subfolders in each for each class. . To start we define the training and test directory paths. . train_dir = &quot;pizza_steak/train/&quot; test_dir = &quot;pizza_steak/test/&quot; . Our next step is to turn our data into batches. . A batch is a small subset of the dataset a model looks at during training. For example, rather than looking at 10,000 images at one time and trying to figure out the patterns, a model might only look at 32 images at a time. . It does this for a couple of reasons: . 10,000 images (or more) might not fit into the memory of your processor (GPU). | Trying to learn the patterns in 10,000 images in one hit could result in the model not being able to learn very well. | . Why 32? . A batch size of 32 is good for your health. . No seriously, there are many different batch sizes you could use but 32 has proven to be very effective in many different use cases and is often the default for many data preprocessing functions. . To turn our data into batches, we&#39;ll first create an instance of ImageDataGenerator for each of our datasets. . from tensorflow.keras.preprocessing.image import ImageDataGenerator train_datagen = ImageDataGenerator(rescale=1/255.) test_datagen = ImageDataGenerator(rescale=1/255.) . The ImageDataGenerator class helps us prepare our images into batches as well as perform transformations on them as they get loaded into the model. . You might&#39;ve noticed the rescale parameter. This is one example of the transformations we&#39;re doing. . Remember from before how we imported an image and it&#39;s pixel values were between 0 and 255? . The rescale parameter, along with 1/255. is like saying &quot;divide all of the pixel values by 255&quot;. This results in all of the image being imported and their pixel values being normalized (converted to be between 0 and 1). . 🔑 Note: For more transformation options such as data augmentation (we&#39;ll see this later), refer to the ImageDataGenerator documentation. Now we&#39;ve got a couple of ImageDataGenerator instances, we can load our images from their respective directories using the flow_from_directory method. . train_data = train_datagen.flow_from_directory(directory=train_dir, target_size=(224, 224), class_mode=&#39;binary&#39;, batch_size=32) test_data = test_datagen.flow_from_directory(directory=test_dir, target_size=(224, 224), class_mode=&#39;binary&#39;, batch_size=32) . Found 1500 images belonging to 2 classes. Found 500 images belonging to 2 classes. . Wonderful! Looks like our training dataset has 1500 images belonging to 2 classes (pizza and steak) and our test dataset has 500 images also belonging to 2 classes. . Some things to here: . Due to how our directories are structured, the classes get inferred by the subdirectory names in train_dir and test_dir. | The target_size parameter defines the input size of our images in (height, width) format. | The class_mode value of &#39;binary&#39; defines our classification problem type. If we had more than two classes, we would use &#39;categorical&#39;. | The batch_size defines how many images will be in each batch, we&#39;ve used 32 which is the same as the default. | . We can take a look at our batched images and labels by inspecting the train_data object. . images, labels = train_data.next() # get the &#39;next&#39; batch of images/labels len(images), len(labels) . (32, 32) . Wonderful, it seems our images and labels are in batches of 32. . Let&#39;s see what the images look like. . images[:2], images[0].shape . (array([[[[0.47058827, 0.40784317, 0.34509805], [0.4784314 , 0.427451 , 0.3647059 ], [0.48627454, 0.43529415, 0.37254903], ..., [0.8313726 , 0.70980394, 0.48627454], [0.8431373 , 0.73333335, 0.5372549 ], [0.87843144, 0.7725491 , 0.5882353 ]], [[0.50980395, 0.427451 , 0.36078432], [0.5058824 , 0.42352945, 0.35686275], [0.5137255 , 0.4431373 , 0.3647059 ], ..., [0.82745105, 0.7058824 , 0.48235297], [0.82745105, 0.70980394, 0.5058824 ], [0.8431373 , 0.73333335, 0.5372549 ]], [[0.5254902 , 0.427451 , 0.34901962], [0.5372549 , 0.43921572, 0.36078432], [0.5372549 , 0.45098042, 0.36078432], ..., [0.82745105, 0.7019608 , 0.4784314 ], [0.82745105, 0.7058824 , 0.49411768], [0.8352942 , 0.7176471 , 0.5137255 ]], ..., [[0.77647066, 0.5647059 , 0.2901961 ], [0.7803922 , 0.53333336, 0.22352943], [0.79215693, 0.5176471 , 0.18039216], ..., [0.30588236, 0.2784314 , 0.24705884], [0.24705884, 0.23137257, 0.19607845], [0.2784314 , 0.27450982, 0.25490198]], [[0.7843138 , 0.57254905, 0.29803923], [0.79215693, 0.54509807, 0.24313727], [0.8000001 , 0.5254902 , 0.18823531], ..., [0.2627451 , 0.23529413, 0.20392159], [0.24313727, 0.227451 , 0.19215688], [0.26666668, 0.2627451 , 0.24313727]], [[0.7960785 , 0.59607846, 0.3372549 ], [0.7960785 , 0.5647059 , 0.26666668], [0.81568635, 0.54901963, 0.22352943], ..., [0.23529413, 0.19607845, 0.16078432], [0.3019608 , 0.26666668, 0.24705884], [0.26666668, 0.2509804 , 0.24705884]]], [[[0.38823533, 0.4666667 , 0.36078432], [0.3921569 , 0.46274513, 0.36078432], [0.38431376, 0.454902 , 0.36078432], ..., [0.5294118 , 0.627451 , 0.54509807], [0.5294118 , 0.627451 , 0.54509807], [0.5411765 , 0.6392157 , 0.5568628 ]], [[0.38431376, 0.454902 , 0.3529412 ], [0.3921569 , 0.46274513, 0.36078432], [0.39607847, 0.4666667 , 0.37254903], ..., [0.54509807, 0.6431373 , 0.5686275 ], [0.5529412 , 0.6509804 , 0.5764706 ], [0.5647059 , 0.6627451 , 0.5882353 ]], [[0.3921569 , 0.46274513, 0.36078432], [0.38431376, 0.454902 , 0.3529412 ], [0.4039216 , 0.47450984, 0.3803922 ], ..., [0.5764706 , 0.67058825, 0.6156863 ], [0.5647059 , 0.6666667 , 0.6156863 ], [0.5647059 , 0.6666667 , 0.6156863 ]], ..., [[0.47058827, 0.5647059 , 0.4784314 ], [0.4784314 , 0.5764706 , 0.4901961 ], [0.48235297, 0.5803922 , 0.49803925], ..., [0.39607847, 0.42352945, 0.3019608 ], [0.37647063, 0.40000004, 0.2901961 ], [0.3803922 , 0.4039216 , 0.3019608 ]], [[0.45098042, 0.5529412 , 0.454902 ], [0.46274513, 0.5647059 , 0.4666667 ], [0.47058827, 0.57254905, 0.47450984], ..., [0.40784317, 0.43529415, 0.3137255 ], [0.39607847, 0.41960788, 0.31764707], [0.38823533, 0.40784317, 0.31764707]], [[0.47450984, 0.5764706 , 0.47058827], [0.47058827, 0.57254905, 0.4666667 ], [0.46274513, 0.5647059 , 0.4666667 ], ..., [0.4039216 , 0.427451 , 0.31764707], [0.3921569 , 0.4156863 , 0.3137255 ], [0.4039216 , 0.42352945, 0.3372549 ]]]], dtype=float32), (224, 224, 3)) . Due to our rescale parameter, the images are now in (224, 224, 3) shape tensors with values between 0 and 1. . How about the labels? . labels . array([1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.], dtype=float32) . Due to the class_mode parameter being &#39;binary&#39; our labels are either 0 (pizza) or 1 (steak). . Now that our data is ready, our model is going to try and figure out the patterns between the image tensors and the labels. . 3. Create a model (start with a baseline) . You might be wondering what your default model architecture should be. . And the truth is, there&#39;s many possible answers to this question. . A simple heuristic for computer vision models is to use the model architecture which is performing best on ImageNet (a large collection of diverse images to benchmark different computer vision models). . However, to begin with, it&#39;s good to build a smaller model to acquire a baseline result which you try to improve upon. . 🔑 Note: In deep learning a smaller model often refers to a model with less layers than the state of the art (SOTA). For example, a smaller model might have 3-4 layers where as a state of the art model, such as, ResNet50 might have 50+ layers. In our case, let&#39;s take a smaller version of the model that can be found on the CNN explainer website (model_1 from above) and build a 3 layer convolutional neural network. . from tensorflow.keras.optimizers import Adam from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Activation from tensorflow.keras import Sequential . model_4 = Sequential([ Conv2D(filters=10, kernel_size=3, strides=1, padding=&#39;valid&#39;, activation=&#39;relu&#39;, input_shape=(224, 224, 3)), # input layer (specify input shape) Conv2D(10, 3, activation=&#39;relu&#39;), Conv2D(10, 3, activation=&#39;relu&#39;), Flatten(), Dense(1, activation=&#39;sigmoid&#39;) # output layer (specify output shape) ]) . Great! We&#39;ve got a simple convolutional neural network architecture ready to go. . And it follows the typical structure of: . # Basic structure of CNN Input -&gt; Conv + ReLU layers (non-linearities) -&gt; Pooling layer -&gt; Fully connected (dense layer) as Output . Let&#39;s discuss some of the components of the Conv2D layer: . The &quot;2D&quot; means our inputs are two dimensional (height and width), even though they have 3 colour channels, the convolutions are run on each channel invididually. | filters - these are the number of &quot;feature extractors&quot; that will be moving over our images. | kernel_size - the size of our filters, for example, a kernel_size of (3, 3) (or just 3) will mean each filter will have the size 3x3, meaning it will look at a space of 3x3 pixels each time. The smaller the kernel, the more fine-grained features it will extract. | stride - the number of pixels a filter will move across as it covers the image. A stride of 1 means the filter moves across each pixel 1 by 1. A stride of 2 means it moves 2 pixels at a time. | padding - this can be either &#39;same&#39; or &#39;valid&#39;, &#39;same&#39; adds zeros the to outside of the image so the resulting output of the convolutional layer is the same as the input, where as &#39;valid&#39; (default) cuts off excess pixels where the filter doesn&#39;t fit (e.g. 224 pixels wide divided by a kernel size of 3 (224/3 = 74.6) means a single pixel will get cut off the end. | . What&#39;s a &quot;feature&quot;? . A feature can be considered any significant part of an image. For example, in our case, a feature might be the circular shape of pizza. Or the rough edges on the outside of a steak. . It&#39;s important to note that these features are not defined by us, instead, the model learns them as it applies different filters across the image. . 📖 Resources: For a great demonstration of these in action, be sure to spend some time going through the following: * CNN Explainer Webpage - a great visual overview of many of the concepts we&#39;re replicating here with code. . A guide to convolutional arithmetic for deep learning - a phenomenal introduction to the math going on behind the scenes of a convolutional neural network. | For a great explanation of padding, see this Stack Overflow answer. | . Now our model is ready, let&#39;s compile it. . model_4.compile(loss=&#39;binary_crossentropy&#39;, optimizer=Adam(), metrics=[&#39;accuracy&#39;]) . Since we&#39;re working on a binary classification problem (pizza vs. steak), the loss function we&#39;re using is &#39;binary_crossentropy&#39;, if it was mult-iclass, we might use something like &#39;categorical_crossentropy&#39;. . Adam with all the default settings is our optimizer and our evaluation metric is accuracy. . 4. Fit a model . Our model is compiled, time to fit it. . You&#39;ll notice two new parameters here: . steps_per_epoch - this is the number of batches a model will go through per epoch, in our case, we want our model to go through all batches so it&#39;s equal to the length of train_data (1500 images in batches of 32 = 1500/32 = ~47 steps) | validation_steps - same as above, except for the validation_data parameter (500 test images in batches of 32 = 500/32 = ~16 steps) | . len(train_data), len(test_data) . (47, 16) . history_4 = model_4.fit(train_data, epochs=5, steps_per_epoch=len(train_data), validation_data=test_data, validation_steps=len(test_data)) . Epoch 1/5 47/47 [==============================] - 10s 196ms/step - loss: 2.2202 - accuracy: 0.5986 - val_loss: 0.4400 - val_accuracy: 0.8040 Epoch 2/5 47/47 [==============================] - 9s 189ms/step - loss: 0.4604 - accuracy: 0.8031 - val_loss: 0.4260 - val_accuracy: 0.8100 Epoch 3/5 47/47 [==============================] - 9s 189ms/step - loss: 0.3505 - accuracy: 0.8616 - val_loss: 0.4287 - val_accuracy: 0.8020 Epoch 4/5 47/47 [==============================] - 9s 190ms/step - loss: 0.1965 - accuracy: 0.9512 - val_loss: 0.4660 - val_accuracy: 0.7920 Epoch 5/5 47/47 [==============================] - 9s 190ms/step - loss: 0.0978 - accuracy: 0.9659 - val_loss: 0.6300 - val_accuracy: 0.7540 . 5. Evaluate the model . Oh yeah! Looks like our model is learning something. . Let&#39;s check out its training curves. . import pandas as pd pd.DataFrame(history_4.history).plot(figsize=(10, 7)); . Hmm, judging by our loss curves, it looks like our model is overfitting the training dataset. . 🔑 Note: When a model&#39;s validation loss starts to increase, it&#39;s likely that it&#39;s overfitting the training dataset. This means, it&#39;s learning the patterns in the training dataset too well and thus its ability to generalize to unseen data will be diminished. To further inspect our model&#39;s training performance, let&#39;s separate the accuracy and loss curves. . def plot_loss_curves(history): &quot;&quot;&quot; Returns separate loss curves for training and validation metrics. &quot;&quot;&quot; loss = history.history[&#39;loss&#39;] val_loss = history.history[&#39;val_loss&#39;] accuracy = history.history[&#39;accuracy&#39;] val_accuracy = history.history[&#39;val_accuracy&#39;] epochs = range(len(history.history[&#39;loss&#39;])) # Plot loss plt.plot(epochs, loss, label=&#39;training_loss&#39;) plt.plot(epochs, val_loss, label=&#39;val_loss&#39;) plt.title(&#39;Loss&#39;) plt.xlabel(&#39;Epochs&#39;) plt.legend() # Plot accuracy plt.figure() plt.plot(epochs, accuracy, label=&#39;training_accuracy&#39;) plt.plot(epochs, val_accuracy, label=&#39;val_accuracy&#39;) plt.title(&#39;Accuracy&#39;) plt.xlabel(&#39;Epochs&#39;) plt.legend(); . plot_loss_curves(history_4) . The ideal position for these two curves is to follow each other. If anything, the validation curve should be slightly under the training curve. If there&#39;s a large gap between the training curve and validation curve, it means your model is probably overfitting. . model_4.summary() . Model: &#34;sequential_3&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_4 (Conv2D) (None, 222, 222, 10) 280 _________________________________________________________________ conv2d_5 (Conv2D) (None, 220, 220, 10) 910 _________________________________________________________________ conv2d_6 (Conv2D) (None, 218, 218, 10) 910 _________________________________________________________________ flatten_3 (Flatten) (None, 475240) 0 _________________________________________________________________ dense_8 (Dense) (None, 1) 475241 ================================================================= Total params: 477,341 Trainable params: 477,341 Non-trainable params: 0 _________________________________________________________________ . 6. Adjust the model parameters . Fitting a machine learning model comes in 3 steps: . Create a basline. | Beat the baseline by overfitting a larger model. | Reduce overfitting. | So far we&#39;ve gone through steps 0 and 1. . And there are even a few more things we could try to further overfit our model: . Increase the number of convolutional layers. | Increase the number of convolutional filters. | Add another dense layer to the output of our flattened layer. | . But what we&#39;ll do instead is focus on getting our model&#39;s training curves to better align with eachother, in other words, we&#39;ll take on step 2. . Why is reducing overfitting important? . When a model performs too well on training data and poorly on unseen data, it&#39;s not much use to us if we wanted to use it in the real world. . Say we were building a pizza vs. steak food classifier app, and our model performs very well on our training data but when users tried it out, they didn&#39;t get very good results on their own food images, is that a good experience? . Not really... . So for the next few models we build, we&#39;re going to adjust a number of parameters and inspect the training curves along the way. . Namely, we&#39;ll build 2 more models: . A ConvNet with max pooling | A ConvNet with max pooling and data augmentation | . For the first model, we&#39;ll follow the structure: . # Basic structure of CNN with max pooling Input -&gt; Conv layers + ReLU layers (non-linearities) + Max Pooling layers -&gt; Fully connected (dense layer) as Output . Let&#39;s built it. It&#39;ll have the same structure as model_4 but with a MaxPool2D() layer after each convolutional layer. . model_5 = Sequential([ Conv2D(10, 3, activation=&#39;relu&#39;, input_shape=(224, 224, 3)), MaxPool2D(pool_size=2), # reduce number of features by half Conv2D(10, 3, activation=&#39;relu&#39;), MaxPool2D(), Conv2D(10, 3, activation=&#39;relu&#39;), MaxPool2D(), Flatten(), Dense(1, activation=&#39;sigmoid&#39;) ]) . Woah, we&#39;ve got another layer type we haven&#39;t seen before. . If convolutional layers learn the features of an image you can think of a Max Pooling layer as figuring out the most important of those features. We&#39;ll see this an example of this in a moment. . model_5.compile(loss=&#39;binary_crossentropy&#39;, optimizer=Adam(), metrics=[&#39;accuracy&#39;]) . history_5 = model_5.fit(train_data, epochs=5, steps_per_epoch=len(train_data), validation_data=test_data, validation_steps=len(test_data)) . Epoch 1/5 47/47 [==============================] - 9s 188ms/step - loss: 0.7124 - accuracy: 0.5322 - val_loss: 0.5493 - val_accuracy: 0.7740 Epoch 2/5 47/47 [==============================] - 8s 180ms/step - loss: 0.5168 - accuracy: 0.7483 - val_loss: 0.4060 - val_accuracy: 0.8060 Epoch 3/5 47/47 [==============================] - 9s 181ms/step - loss: 0.4644 - accuracy: 0.7915 - val_loss: 0.3468 - val_accuracy: 0.8620 Epoch 4/5 47/47 [==============================] - 9s 182ms/step - loss: 0.4004 - accuracy: 0.8304 - val_loss: 0.3423 - val_accuracy: 0.8680 Epoch 5/5 47/47 [==============================] - 9s 182ms/step - loss: 0.3684 - accuracy: 0.8543 - val_loss: 0.3235 - val_accuracy: 0.8500 . Okay, it looks like our model with max pooling (model_5) is performing worse on the training set but better on the validation set. . Before we checkout its training curves, let&#39;s check out its architecture. . model_5.summary() . Model: &#34;sequential_4&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_7 (Conv2D) (None, 222, 222, 10) 280 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 111, 111, 10) 0 _________________________________________________________________ conv2d_8 (Conv2D) (None, 109, 109, 10) 910 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 54, 54, 10) 0 _________________________________________________________________ conv2d_9 (Conv2D) (None, 52, 52, 10) 910 _________________________________________________________________ max_pooling2d_4 (MaxPooling2 (None, 26, 26, 10) 0 _________________________________________________________________ flatten_4 (Flatten) (None, 6760) 0 _________________________________________________________________ dense_9 (Dense) (None, 1) 6761 ================================================================= Total params: 8,861 Trainable params: 8,861 Non-trainable params: 0 _________________________________________________________________ . Do you notice what&#39;s going on here with the output shape in each MaxPooling2D layer? . It gets halved each time. This is effectively the MaxPooling2D layer taking the outputs of each Conv2D layer and saying &quot;I only want the most important features, get rid of the rest&quot;. . The bigger the pool_size parameter, the more the max pooling layer will squeeze the features out of the image. However, too big and the model might not be able to learn anything. . The results of this pooling are seen in a major reduction of total trainable parameters (8,861 in model_5 and 477,431 in model_4). . Time to check out the loss curves. . plot_loss_curves(history_5) . Nice! We can see the training curves get a lot closer to eachother. However, our the validation loss looks to start increasing towards the end and in turn potentially leading to overfitting. . Time to dig into our bag of tricks and try another method of overfitting prevention, data augmentation. . First, we&#39;ll see how it&#39;s done with code then we&#39;ll discuss what it&#39;s doing. . To implement data augmentation, we&#39;ll have to reinstantiate our ImageDataGenerator instances. . train_datagen_augmented = ImageDataGenerator(rescale=1/255., rotation_range=0.2, # rotate the image slightly shear_range=0.2, # shear the image zoom_range=0.2, # zoom into the image width_shift_range=0.2, # shift the image width ways height_shift_range=0.2, # shift the image height ways horizontal_flip=True) # flip the image on the horizontal axis # Create ImageDataGenerator training instance without data augmentation train_datagen = ImageDataGenerator(rescale=1/255.) # Create ImageDataGenerator test instance without data augmentation test_datagen = ImageDataGenerator(rescale=1/255.) . 🤔 Question: What&#39;s data augmentation? Data augmentation is the process of altering our training data, leading to it having more diversity and in turn allowing our models to learn more generalizable patterns. Altering might mean adjusting the rotation of an image, flipping it, cropping it or something similar. . Doing this simulates the kind of data a model might be used on in the real world. . If we&#39;re building a pizza vs. steak application, not all of the images our users take might be in similar setups to our training data. Using data augmentation gives us another way to prevent overfitting and in turn make our model more generalizable. . 🔑 Note: Data augmentation is usally only performed on the training data. Using the ImageDataGenerator built-in data augmentation parameters our images are left as they are in the directories but are randomly manipulated when loaded into the model. . print(&quot;Augmented training images:&quot;) train_data_augmented = train_datagen_augmented.flow_from_directory(train_dir, target_size=(224, 224), batch_size=32, class_mode=&#39;binary&#39;, shuffle=False) # Don&#39;t shuffle for demonstration purposes, usually a good thing to shuffle # Create non-augmented data batches print(&quot;Non-augmented training images:&quot;) train_data = train_datagen.flow_from_directory(train_dir, target_size=(224, 224), batch_size=32, class_mode=&#39;binary&#39;, shuffle=False) # Don&#39;t shuffle for demonstration purposes print(&quot;Unchanged test images:&quot;) test_data = test_datagen.flow_from_directory(test_dir, target_size=(224, 224), batch_size=32, class_mode=&#39;binary&#39;) . Augmented training images: Found 1500 images belonging to 2 classes. Non-augmented training images: Found 1500 images belonging to 2 classes. Unchanged test images: Found 500 images belonging to 2 classes. . Better than talk about data augmentation, how about we see it? . (remember our motto? visualize, visualize, visualize...) . images, labels = train_data.next() augmented_images, augmented_labels = train_data_augmented.next() # Note: labels aren&#39;t augmented, they stay the same . random_number = random.randint(0, 32) # we&#39;re making batches of size 32, so we&#39;ll get a random instance plt.imshow(images[random_number]) plt.title(f&quot;Original image&quot;) plt.axis(False) plt.figure() plt.imshow(augmented_images[random_number]) plt.title(f&quot;Augmented image&quot;) plt.axis(False); . After going through a sample of original and augmented images, you can start to see some of the example transformations on the training images. . Notice how some of the augmented images look like slightly warped versions of the original image. This means our model will be forced to try and learn patterns in less-than-perfect images, which is often the case when using real-world images. . 🤔 Question: Should I use data augmentation? And how much should I augment? Data augmentation is a way to try and prevent a model overfitting. If your model is overfiting (e.g. the validation loss keeps increasing), you may want to try using data augmentation. . As for how much to data augment, there&#39;s no set practice for this. Best to check out the options in the ImageDataGenerator class and think about how a model in your use case might benefit from some data augmentation. . Now we&#39;ve got augmented data, let&#39;s try and refit a model on it and see how it affects training. . We&#39;ll use the same model as model_5. . model_6 = Sequential([ Conv2D(10, 3, activation=&#39;relu&#39;, input_shape=(224, 224, 3)), MaxPool2D(pool_size=2), # reduce number of features by half Conv2D(10, 3, activation=&#39;relu&#39;), MaxPool2D(), Conv2D(10, 3, activation=&#39;relu&#39;), MaxPool2D(), Flatten(), Dense(1, activation=&#39;sigmoid&#39;) ]) # Compile the model model_6.compile(loss=&#39;binary_crossentropy&#39;, optimizer=Adam(), metrics=[&#39;accuracy&#39;]) # Fit the model history_6 = model_6.fit(train_data_augmented, # changed to augmented training data epochs=5, steps_per_epoch=len(train_data_augmented), validation_data=test_data, validation_steps=len(test_data)) . Epoch 1/5 47/47 [==============================] - 22s 455ms/step - loss: 0.7150 - accuracy: 0.4305 - val_loss: 0.6854 - val_accuracy: 0.5060 Epoch 2/5 47/47 [==============================] - 21s 451ms/step - loss: 0.6940 - accuracy: 0.4982 - val_loss: 0.6849 - val_accuracy: 0.5940 Epoch 3/5 47/47 [==============================] - 21s 451ms/step - loss: 0.6941 - accuracy: 0.4960 - val_loss: 0.6782 - val_accuracy: 0.5340 Epoch 4/5 47/47 [==============================] - 21s 450ms/step - loss: 0.6901 - accuracy: 0.5251 - val_loss: 0.6343 - val_accuracy: 0.7440 Epoch 5/5 47/47 [==============================] - 21s 448ms/step - loss: 0.6744 - accuracy: 0.6188 - val_loss: 0.5901 - val_accuracy: 0.7000 . 🤔 Question: Why didn&#39;t our model get very good results on the training set to begin with? It&#39;s because when we created train_data_augmented we turned off data shuffling using shuffle=False which means our model only sees a batch of a single kind of images at a time. . For example, the pizza class gets loaded in first because it&#39;s the first class. Thus it&#39;s performance is measured on only a single class rather than both classes. The validation data performance improves steadily because it contains shuffled data. . Since we only set shuffle=False for demonstration purposes (so we could plot the same augmented and non-augmented image), we can fix this by setting shuffle=True on future data generators. . You may have also noticed each epoch taking longer when training with augmented data compared to when training with non-augmented data (~25s per epoch vs. ~10s per epoch). . This is because the ImageDataGenerator instance augments the data as it&#39;s loaded into the model. The benefit of this is that it leaves the original images unchanged. The downside is that it takes longer to load them in. . 🔑 Note: One possible method to speed up dataset manipulation would be to look into TensorFlow&#39;s parrallel reads and buffered prefecting options. . plot_loss_curves(history_6) . It seems our validation loss curve is heading in the right direction but it&#39;s a bit jumpy (the most ideal loss curve isn&#39;t too spiky but a smooth descent, however, a perfectly smooth loss curve is the equivalent of a fairytale). . Let&#39;s see what happens when we shuffle the augmented training data. . train_data_augmented_shuffled = train_datagen_augmented.flow_from_directory(train_dir, target_size=(224, 224), batch_size=32, class_mode=&#39;binary&#39;, shuffle=True) # Shuffle data (default) . Found 1500 images belonging to 2 classes. . model_7 = Sequential([ Conv2D(10, 3, activation=&#39;relu&#39;, input_shape=(224, 224, 3)), MaxPool2D(), Conv2D(10, 3, activation=&#39;relu&#39;), MaxPool2D(), Conv2D(10, 3, activation=&#39;relu&#39;), MaxPool2D(), Flatten(), Dense(1, activation=&#39;sigmoid&#39;) ]) # Compile the model model_7.compile(loss=&#39;binary_crossentropy&#39;, optimizer=Adam(), metrics=[&#39;accuracy&#39;]) # Fit the model history_7 = model_7.fit(train_data_augmented_shuffled, # now the augmented data is shuffled epochs=5, steps_per_epoch=len(train_data_augmented_shuffled), validation_data=test_data, validation_steps=len(test_data)) . Epoch 1/5 47/47 [==============================] - 22s 457ms/step - loss: 0.6802 - accuracy: 0.5420 - val_loss: 0.5414 - val_accuracy: 0.7160 Epoch 2/5 47/47 [==============================] - 21s 450ms/step - loss: 0.5663 - accuracy: 0.7112 - val_loss: 0.4458 - val_accuracy: 0.7980 Epoch 3/5 47/47 [==============================] - 21s 448ms/step - loss: 0.5206 - accuracy: 0.7290 - val_loss: 0.3785 - val_accuracy: 0.8380 Epoch 4/5 47/47 [==============================] - 21s 449ms/step - loss: 0.4890 - accuracy: 0.7737 - val_loss: 0.3828 - val_accuracy: 0.8400 Epoch 5/5 47/47 [==============================] - 21s 455ms/step - loss: 0.4936 - accuracy: 0.7736 - val_loss: 0.4715 - val_accuracy: 0.7560 . plot_loss_curves(history_7) . Notice with model_7 how the performance on the training dataset improves almost immediately compared to model_6. This is because we shuffled the training data as we passed it to the model using the parameter shuffle=True in the flow_from_directory method. . This means the model was able to see examples of both pizza and steak images in each batch and in turn be evaluated on what it learned from both images rather than just one kind. . Also, our loss curves look a little bit smoother with shuffled data (comparing history_6 to history_7). . 7. Repeat until satisified . We&#39;ve trained a few model&#39;s on our dataset already and so far they&#39;re performing pretty good. . Since we&#39;ve already beaten our baseline, there are a few things we could try to continue to improve our model: . Increase the number of model layers (e.g. add more convolutional layers). | Increase the number of filters in each convolutional layer (e.g. from 10 to 32, 64, or 128, these numbers aren&#39;t set in stone either, they are usually found through trial and error). | Train for longer (more epochs). | Finding an ideal learning rate. | Get more data (give the model more opportunities to learn). | Use transfer learning to leverage what another image model has learned and adjust it for our own use case. | . Adjusting each of these settings (except for the last two) during model development is usually referred to as hyperparameter tuning. . You can think of hyperparameter tuning as simialr to adjusting the settings on your oven to cook your favourite dish. Although your oven does most of the cooking for you, you can help it by tweaking the dials. . Let&#39;s go back to right where we started and try our original model (model_1 or the TinyVGG architecture from CNN explainer). . model_8 = Sequential([ Conv2D(10, 3, activation=&#39;relu&#39;, input_shape=(224, 224, 3)), # same input shape as our images Conv2D(10, 3, activation=&#39;relu&#39;), MaxPool2D(), Conv2D(10, 3, activation=&#39;relu&#39;), Conv2D(10, 3, activation=&#39;relu&#39;), MaxPool2D(), Flatten(), Dense(1, activation=&#39;sigmoid&#39;) ]) # Compile the model model_8.compile(loss=&quot;binary_crossentropy&quot;, optimizer=tf.keras.optimizers.Adam(), metrics=[&quot;accuracy&quot;]) # Fit the model history_8 = model_8.fit(train_data_augmented_shuffled, epochs=5, steps_per_epoch=len(train_data_augmented_shuffled), validation_data=test_data, validation_steps=len(test_data)) . Epoch 1/5 47/47 [==============================] - 22s 466ms/step - loss: 0.7158 - accuracy: 0.5312 - val_loss: 0.5961 - val_accuracy: 0.6940 Epoch 2/5 47/47 [==============================] - 22s 460ms/step - loss: 0.5920 - accuracy: 0.6836 - val_loss: 0.3981 - val_accuracy: 0.8260 Epoch 3/5 47/47 [==============================] - 22s 459ms/step - loss: 0.5171 - accuracy: 0.7553 - val_loss: 0.4007 - val_accuracy: 0.8180 Epoch 4/5 47/47 [==============================] - 22s 458ms/step - loss: 0.4789 - accuracy: 0.7813 - val_loss: 0.3665 - val_accuracy: 0.8440 Epoch 5/5 47/47 [==============================] - 22s 459ms/step - loss: 0.4844 - accuracy: 0.7873 - val_loss: 0.3686 - val_accuracy: 0.8240 . 🔑 Note: You might&#39;ve noticed we used some slightly different code to build model_8 as compared to model_1. This is because of the imports we did before, such as from tensorflow.keras.layers import Conv2D reduce the amount of code we had to write. Although the code is different, the architectures are the same. . model_1.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 222, 222, 10) 280 _________________________________________________________________ conv2d_1 (Conv2D) (None, 220, 220, 10) 910 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 110, 110, 10) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 108, 108, 10) 910 _________________________________________________________________ conv2d_3 (Conv2D) (None, 106, 106, 10) 910 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 53, 53, 10) 0 _________________________________________________________________ flatten (Flatten) (None, 28090) 0 _________________________________________________________________ dense (Dense) (None, 1) 28091 ================================================================= Total params: 31,101 Trainable params: 31,101 Non-trainable params: 0 _________________________________________________________________ . model_8.summary() . Model: &#34;sequential_8&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_19 (Conv2D) (None, 222, 222, 10) 280 _________________________________________________________________ conv2d_20 (Conv2D) (None, 220, 220, 10) 910 _________________________________________________________________ max_pooling2d_14 (MaxPooling (None, 110, 110, 10) 0 _________________________________________________________________ conv2d_21 (Conv2D) (None, 108, 108, 10) 910 _________________________________________________________________ conv2d_22 (Conv2D) (None, 106, 106, 10) 910 _________________________________________________________________ max_pooling2d_15 (MaxPooling (None, 53, 53, 10) 0 _________________________________________________________________ flatten_8 (Flatten) (None, 28090) 0 _________________________________________________________________ dense_13 (Dense) (None, 1) 28091 ================================================================= Total params: 31,101 Trainable params: 31,101 Non-trainable params: 0 _________________________________________________________________ . Now let&#39;s check out our TinyVGG model&#39;s performance. . plot_loss_curves(history_8) . plot_loss_curves(history_1) . Hmm, our training curves are looking good, but our model&#39;s performance on the training and test sets didn&#39;t improve much compared to the previous model. . Taking another loook at the training curves, it looks like our model&#39;s performance might improve if we trained it a little longer (more epochs). . Perhaps that&#39;s something you like to try? . Making a prediction with our trained model . What good is a trained model if you can&#39;t make predictions with it? . To really test it out, we&#39;ll upload a couple of our own images and see how the model goes. . First, let&#39;s remind ourselves of the classnames and view the image we&#39;re going to test on. . print(class_names) . [&#39;pizza&#39; &#39;steak&#39;] . The first test image we&#39;re going to use is a delicious steak I cooked the other day. . !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-steak.jpeg steak = mpimg.imread(&quot;03-steak.jpeg&quot;) plt.imshow(steak) plt.axis(False); . --2021-01-11 04:35:18-- https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-steak.jpeg Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 1978213 (1.9M) [image/jpeg] Saving to: ‘03-steak.jpeg’ 03-steak.jpeg 100%[===================&gt;] 1.89M --.-KB/s in 0.04s 2021-01-11 04:35:19 (49.8 MB/s) - ‘03-steak.jpeg’ saved [1978213/1978213] . steak.shape . (4032, 3024, 3) . Since our model takes in images of shapes (224, 224, 3), we&#39;ve got to reshape our custom image to use it with our model. . To do so, we can import and decode our image using tf.io.read_file (for readining files) and tf.image (for resizing our image and turning it into a tensor). . 🔑 Note: For your model to make predictions on unseen data, for example, your own custom images, the custom image has to be in the same shape as your model has been trained on. In more general terms, to make predictions on custom data it has to be in the same form that your model has been trained on. . def load_and_prep_image(filename, img_shape=224): &quot;&quot;&quot; Reads in an image from filename, turns it into a tensor and reshapes into (224, 224, 3). &quot;&quot;&quot; # Read in the image img = tf.io.read_file(filename) # Decode it into a tensor img = tf.image.decode_jpeg(img) # Resize the image img = tf.image.resize(img, [img_shape, img_shape]) # Rescale the image (get all values between 0 and 1) img = img/255. return img . Now we&#39;ve got a function to load our custom image, let&#39;s load it in. . steak = load_and_prep_image(&quot;03-steak.jpeg&quot;) steak . &lt;tf.Tensor: shape=(224, 224, 3), dtype=float32, numpy= array([[[0.6377451 , 0.6220588 , 0.57892156], [0.6504902 , 0.63186276, 0.5897059 ], [0.63186276, 0.60833335, 0.5612745 ], ..., [0.52156866, 0.05098039, 0.09019608], [0.49509802, 0.04215686, 0.07058824], [0.52843136, 0.07745098, 0.10490196]], [[0.6617647 , 0.6460784 , 0.6107843 ], [0.6387255 , 0.6230392 , 0.57598037], [0.65588236, 0.63235295, 0.5852941 ], ..., [0.5352941 , 0.06862745, 0.09215686], [0.529902 , 0.05931373, 0.09460784], [0.5142157 , 0.05539216, 0.08676471]], [[0.6519608 , 0.6362745 , 0.5892157 ], [0.6392157 , 0.6137255 , 0.56764704], [0.65637255, 0.6269608 , 0.5828431 ], ..., [0.53137255, 0.06470589, 0.08039216], [0.527451 , 0.06862745, 0.1 ], [0.52254903, 0.05196078, 0.0872549 ]], ..., [[0.49313724, 0.42745098, 0.31029412], [0.05441177, 0.01911765, 0. ], [0.2127451 , 0.16176471, 0.09509804], ..., [0.6132353 , 0.59362745, 0.57009804], [0.65294117, 0.6333333 , 0.6098039 ], [0.64166665, 0.62990195, 0.59460783]], [[0.65392154, 0.5715686 , 0.45 ], [0.6367647 , 0.54656863, 0.425 ], [0.04656863, 0.01372549, 0. ], ..., [0.6372549 , 0.61764705, 0.59411764], [0.63529414, 0.6215686 , 0.5892157 ], [0.6401961 , 0.62058824, 0.59705883]], [[0.1 , 0.05539216, 0. ], [0.48333332, 0.40882352, 0.29117647], [0.65 , 0.5686275 , 0.44019607], ..., [0.6308824 , 0.6161765 , 0.5808824 ], [0.6519608 , 0.63186276, 0.5901961 ], [0.6338235 , 0.6259804 , 0.57892156]]], dtype=float32)&gt; . Wonderful, our image is in tensor format, time to try it with our model! . model_8.predict(steak) . ValueError Traceback (most recent call last) &lt;ipython-input-76-fd7eef5274d1&gt; in &lt;module&gt;() 1 # Make a prediction on our custom image (spoiler: this won&#39;t work) -&gt; 2 model_8.predict(steak) /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing) 1627 for step in data_handler.steps(): 1628 callbacks.on_predict_batch_begin(step) -&gt; 1629 tmp_batch_outputs = self.predict_function(iterator) 1630 if data_handler.should_sync: 1631 context.async_wait() /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds) 826 tracing_count = self.experimental_get_tracing_count() 827 with trace.Trace(self._name) as tm: --&gt; 828 result = self._call(*args, **kwds) 829 compiler = &#34;xla&#34; if self._experimental_compile else &#34;nonXla&#34; 830 new_tracing_count = self.experimental_get_tracing_count() /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds) 860 # In this case we have not created variables on the first call. So we can 861 # run the first trace but we should fail if variables are created. --&gt; 862 results = self._stateful_fn(*args, **kwds) 863 if self._created_variables: 864 raise ValueError(&#34;Creating variables on a non-first call to a function&#34; /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs) 2939 with self._lock: 2940 (graph_function, -&gt; 2941 filtered_flat_args) = self._maybe_define_function(args, kwargs) 2942 return graph_function._call_flat( 2943 filtered_flat_args, captured_inputs=graph_function.captured_inputs) # pylint: disable=protected-access /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs) 3356 call_context_key in self._function_cache.missed): 3357 return self._define_function_with_shape_relaxation( -&gt; 3358 args, kwargs, flat_args, filtered_flat_args, cache_key_context) 3359 3360 self._function_cache.missed.add(call_context_key) /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _define_function_with_shape_relaxation(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context) 3278 3279 graph_function = self._create_graph_function( -&gt; 3280 args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes) 3281 self._function_cache.arg_relaxed[rank_only_cache_key] = graph_function 3282 /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes) 3204 arg_names=arg_names, 3205 override_flat_arg_shapes=override_flat_arg_shapes, -&gt; 3206 capture_by_value=self._capture_by_value), 3207 self._function_attributes, 3208 function_spec=self.function_spec, /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes) 988 _, original_func = tf_decorator.unwrap(python_func) 989 --&gt; 990 func_outputs = python_func(*func_args, **func_kwargs) 991 992 # invariant: `func_outputs` contains only Tensors, CompositeTensors, /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds) 632 xla_context.Exit() 633 else: --&gt; 634 out = weak_wrapped_fn().__wrapped__(*args, **kwds) 635 return out 636 /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs) 975 except Exception as e: # pylint:disable=broad-except 976 if hasattr(e, &#34;ag_error_metadata&#34;): --&gt; 977 raise e.ag_error_metadata.to_exception(e) 978 else: 979 raise ValueError: in user code: /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1478 predict_function * return step_function(self, iterator) /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1468 step_function ** outputs = model.distribute_strategy.run(run_step, args=(data,)) /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs) /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica return self._call_for_each_replica(fn, args, kwargs) /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica return fn(*args, **kwargs) /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1461 run_step ** outputs = model.predict_step(data) /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1434 predict_step return self(x, training=False) /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__ input_spec.assert_input_compatibility(self.input_spec, inputs, self.name) /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py:239 assert_input_compatibility str(tuple(shape))) ValueError: Input 0 of layer sequential_8 is incompatible with the layer: : expected min_ndim=4, found ndim=3. Full shape received: (32, 224, 3) . There&#39;s one more problem... . Although our image is in the same shape as the images our model has been trained on, we&#39;re still missing a dimension. . Remember how our model was trained in batches? . Well, the batch size becomes the first dimension. . So in reality, our model was trained on data in the shape of (batch_size, 224, 224, 3). . We can fix this by adding an extra to our custom image tensor using tf.expand_dims. . print(f&quot;Shape before new dimension: {steak.shape}&quot;) steak = tf.expand_dims(steak, axis=0) # add an extra dimension at axis 0 #steak = steak[tf.newaxis, ...] # alternative to the above, &#39;...&#39; is short for &#39;every other dimension&#39; print(f&quot;Shape after new dimension: {steak.shape}&quot;) steak . Shape before new dimension: (224, 224, 3) Shape after new dimension: (1, 224, 224, 3) . &lt;tf.Tensor: shape=(1, 224, 224, 3), dtype=float32, numpy= array([[[[0.6377451 , 0.6220588 , 0.57892156], [0.6504902 , 0.63186276, 0.5897059 ], [0.63186276, 0.60833335, 0.5612745 ], ..., [0.52156866, 0.05098039, 0.09019608], [0.49509802, 0.04215686, 0.07058824], [0.52843136, 0.07745098, 0.10490196]], [[0.6617647 , 0.6460784 , 0.6107843 ], [0.6387255 , 0.6230392 , 0.57598037], [0.65588236, 0.63235295, 0.5852941 ], ..., [0.5352941 , 0.06862745, 0.09215686], [0.529902 , 0.05931373, 0.09460784], [0.5142157 , 0.05539216, 0.08676471]], [[0.6519608 , 0.6362745 , 0.5892157 ], [0.6392157 , 0.6137255 , 0.56764704], [0.65637255, 0.6269608 , 0.5828431 ], ..., [0.53137255, 0.06470589, 0.08039216], [0.527451 , 0.06862745, 0.1 ], [0.52254903, 0.05196078, 0.0872549 ]], ..., [[0.49313724, 0.42745098, 0.31029412], [0.05441177, 0.01911765, 0. ], [0.2127451 , 0.16176471, 0.09509804], ..., [0.6132353 , 0.59362745, 0.57009804], [0.65294117, 0.6333333 , 0.6098039 ], [0.64166665, 0.62990195, 0.59460783]], [[0.65392154, 0.5715686 , 0.45 ], [0.6367647 , 0.54656863, 0.425 ], [0.04656863, 0.01372549, 0. ], ..., [0.6372549 , 0.61764705, 0.59411764], [0.63529414, 0.6215686 , 0.5892157 ], [0.6401961 , 0.62058824, 0.59705883]], [[0.1 , 0.05539216, 0. ], [0.48333332, 0.40882352, 0.29117647], [0.65 , 0.5686275 , 0.44019607], ..., [0.6308824 , 0.6161765 , 0.5808824 ], [0.6519608 , 0.63186276, 0.5901961 ], [0.6338235 , 0.6259804 , 0.57892156]]]], dtype=float32)&gt; . Our custom image has a batch size of 1! Let&#39;s make a prediction on it. . pred = model_8.predict(steak) pred . array([[0.7130781]], dtype=float32) . Ahh, the predictions come out in prediction probability form. In other words, this means how likely the image is to be one class or another. . Since we&#39;re working with a binary classification problem, if the prediction probability is over 0.5, according to the model, the prediction is most likely to be the postive class (class 1). . And if the prediction probability is under 0.5, according to the model, the predicted class is most likely to be the negative class (class 0). . 🔑 Note: The 0.5 cutoff can be adjusted to your liking. For example, you could set the limit to be 0.8 and over for the positive class and 0.2 for the negative class. However, doing this will almost always change your model&#39;s performance metrics so be sure to make sure they change in the right direction. But saying positive and negative class doesn&#39;t make much sense when we&#39;re working with pizza 🍕 and steak 🥩... . So let&#39;s write a little function to convert predictions into their class names and then plot the target image. . class_names . array([&#39;pizza&#39;, &#39;steak&#39;], dtype=&#39;&lt;U5&#39;) . pred_class = class_names[int(tf.round(pred)[0][0])] pred_class . &#39;steak&#39; . def pred_and_plot(model, filename, class_names): &quot;&quot;&quot; Imports an image located at filename, makes a prediction on it with a trained model and plots the image with the predicted class as the title. &quot;&quot;&quot; # Import the target image and preprocess it img = load_and_prep_image(filename) # Make a prediction pred = model.predict(tf.expand_dims(img, axis=0)) # Get the predicted class pred_class = class_names[int(tf.round(pred)[0][0])] # Plot the image and predicted class plt.imshow(img) plt.title(f&quot;Prediction: {pred_class}&quot;) plt.axis(False); . pred_and_plot(model_8, &quot;03-steak.jpeg&quot;, class_names) . Nice! Our model got the prediction right. . The only downside of working with food is this is making me hungry. . Let&#39;s try one more image. . !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-pizza-dad.jpeg pred_and_plot(model_8, &quot;03-pizza-dad.jpeg&quot;, class_names) . --2021-01-11 04:42:27-- https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-pizza-dad.jpeg Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 2874848 (2.7M) [image/jpeg] Saving to: ‘03-pizza-dad.jpeg.1’ 03-pizza-dad.jpeg.1 100%[===================&gt;] 2.74M --.-KB/s in 0.05s 2021-01-11 04:42:27 (58.3 MB/s) - ‘03-pizza-dad.jpeg.1’ saved [2874848/2874848] . Two thumbs up! Woohoo! . Multi-class Classification . We&#39;ve referenced the TinyVGG architecture from the CNN Explainer website multiple times through this notebook, however, the CNN Explainer website works with 10 different image classes, where as our current model only works with two classes (pizza and steak). . 🛠 Practice: Before scrolling down, how do you think we might change our model to work with 10 classes of the same kind of images? Assume the data is in the same style as our two class problem. Remember the steps we took before to build our pizza 🍕 vs. steak 🥩 classifier? . How about we go through those steps again, except this time, we&#39;ll work with 10 different types of food. . Become one with the data (visualize, visualize, visualize...) | Preprocess the data (prepare it for a model) | Create a model (start with a baseline) | Fit the model | Evaluate the model | Adjust different parameters and improve model (try to beat your baseline) | Repeat until satisfied | The workflow we&#39;re about to go through is a slightly modified version of the above image. As you keep going through deep learning problems, you&#39;ll find the workflow above is more of an outline than a step-by-step guide. . 1. Import and become one with the data . Again, we&#39;ve got a subset of the Food101 dataset. In addition to the pizza and steak images, we&#39;ve pulled out another eight classes. . import zipfile # Download zip file of 10_food_classes images # See how this data was created - https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/image_data_modification.ipynb !wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip # Unzip the downloaded file zip_ref = zipfile.ZipFile(&quot;10_food_classes_all_data.zip&quot;, &quot;r&quot;) zip_ref.extractall() zip_ref.close() . Now let&#39;s check out all of the different directories and sub-directories in the 10_food_classes file. . import os # Walk through 10_food_classes directory and list number of files for dirpath, dirnames, filenames in os.walk(&quot;10_food_classes_all_data&quot;): print(f&quot;There are {len(dirnames)} directories and {len(filenames)} images in &#39;{dirpath}&#39;.&quot;) . There are 2 directories and 1 images in &#39;10_food_classes_all_data&#39;. There are 10 directories and 0 images in &#39;10_food_classes_all_data/train&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/pizza&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/hamburger&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/steak&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/grilled_salmon&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/chicken_curry&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/ramen&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/ice_cream&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/fried_rice&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/sushi&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/chicken_wings&#39;. There are 10 directories and 0 images in &#39;10_food_classes_all_data/test&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/pizza&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/hamburger&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/steak&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/grilled_salmon&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/chicken_curry&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/ramen&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/ice_cream&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/fried_rice&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/sushi&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/chicken_wings&#39;. . Looking good! . We&#39;ll now setup the training and test directory paths. . train_dir = &quot;10_food_classes_all_data/train/&quot; test_dir = &quot;10_food_classes_all_data/test/&quot; . And get the class names from the subdirectories. . import pathlib import numpy as np data_dir = pathlib.Path(train_dir) class_names = np.array(sorted([item.name for item in data_dir.glob(&#39;*&#39;)])) print(class_names) . [&#39;chicken_curry&#39; &#39;chicken_wings&#39; &#39;fried_rice&#39; &#39;grilled_salmon&#39; &#39;hamburger&#39; &#39;ice_cream&#39; &#39;pizza&#39; &#39;ramen&#39; &#39;steak&#39; &#39;sushi&#39;] . How about we visualize an image from the training set? . import random img = view_random_image(target_dir=train_dir, target_class=random.choice(class_names)) # get a random class name . Image shape: (307, 512, 3) . 2. Preprocess the data (prepare it for a model) . After going through a handful of images (it&#39;s good to visualize at least 10-100 different examples), it looks like our data directories are setup correctly. . Time to preprocess the data. . from tensorflow.keras.preprocessing.image import ImageDataGenerator # Rescale the data and create data generator instances train_datagen = ImageDataGenerator(rescale=1/255.) test_datagen = ImageDataGenerator(rescale=1/255.) # Load data in from directories and turn it into batches train_data = train_datagen.flow_from_directory(train_dir, target_size=(224, 224), batch_size=32, class_mode=&#39;categorical&#39;) # changed to categorical test_data = train_datagen.flow_from_directory(test_dir, target_size=(224, 224), batch_size=32, class_mode=&#39;categorical&#39;) . Found 7500 images belonging to 10 classes. Found 2500 images belonging to 10 classes. . As with binary classifcation, we&#39;ve creator image generators. The main change this time is that we&#39;ve changed the class_mode parameter to &#39;categorical&#39; because we&#39;re dealing with 10 classes of food images. . Everything else like rescaling the images, creating the batch size and target image size stay the same. . 🤔 Question: Why is the image size 224x224? This could actually be any size we wanted, however, 224x224 is a very common size for preprocessing images to. Depending on your problem you might want to use larger or smaller images. . 3. Create a model (start with a baseline) . We can use the same model (TinyVGG) we used for the binary classification problem for our multi-class classification problem with a couple of small tweaks. . Namely: . Changing the output layer to use have 10 ouput neurons (the same number as the number of classes we have). | Changing the output layer to use &#39;softmax&#39; activation instead of &#39;sigmoid&#39; activation. | Changing the loss function to be &#39;categorical_crossentropy&#39; instead of &#39;binary_crossentropy&#39;. | . import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense # Create our model (a clone of model_8, except to be multi-class) model_9 = Sequential([ Conv2D(10, 3, activation=&#39;relu&#39;, input_shape=(224, 224, 3)), Conv2D(10, 3, activation=&#39;relu&#39;), MaxPool2D(), Conv2D(10, 3, activation=&#39;relu&#39;), Conv2D(10, 3, activation=&#39;relu&#39;), MaxPool2D(), Flatten(), Dense(10, activation=&#39;softmax&#39;) # changed to have 10 neurons (same as number of classes) and &#39;softmax&#39; activation ]) # Compile the model model_9.compile(loss=&quot;categorical_crossentropy&quot;, # changed to categorical_crossentropy optimizer=tf.keras.optimizers.Adam(), metrics=[&quot;accuracy&quot;]) . 4. Fit a model . Now we&#39;ve got a model suited for working with multiple classes, let&#39;s fit it to our data. . history_9 = model_9.fit(train_data, # now 10 different classes epochs=5, steps_per_epoch=len(train_data), validation_data=test_data, validation_steps=len(test_data)) . Epoch 1/5 235/235 [==============================] - 44s 188ms/step - loss: 2.1403 - accuracy: 0.2188 - val_loss: 2.0135 - val_accuracy: 0.2760 Epoch 2/5 235/235 [==============================] - 44s 187ms/step - loss: 1.8895 - accuracy: 0.3452 - val_loss: 1.9687 - val_accuracy: 0.3060 Epoch 3/5 235/235 [==============================] - 43s 185ms/step - loss: 1.5193 - accuracy: 0.4855 - val_loss: 2.0796 - val_accuracy: 0.3104 Epoch 4/5 235/235 [==============================] - 44s 185ms/step - loss: 0.9673 - accuracy: 0.6849 - val_loss: 2.3800 - val_accuracy: 0.3004 Epoch 5/5 235/235 [==============================] - 44s 186ms/step - loss: 0.4256 - accuracy: 0.8673 - val_loss: 3.3508 - val_accuracy: 0.2588 . Why do you think each epoch takes longer than when working with only two classes of images? . It&#39;s because we&#39;re now dealing with more images than we were before. We&#39;ve got 10 classes with 750 training images and 250 validation images each totalling 10,000 images. Where as when we had two classes, we had 1500 training images and 500 validation images, totalling 2000. . The intuitive reasoning here is the more data you have, the longer a model will take to find patterns. . 5. Evaluate the model . Woohoo! We&#39;ve just trained a model on 10 different classes of food images, let&#39;s see how it went. . model_9.evaluate(test_data) . 79/79 [==============================] - 10s 130ms/step - loss: 3.3508 - accuracy: 0.2588 . [3.350841760635376, 0.2587999999523163] . plot_loss_curves(history_9) . Woah, that&#39;s quite the gap between the training and validation loss curves. . What does this tell us? . It seems our model is overfitting the training set quite badly. In other words, it&#39;s getting great results on the training data but fails to generalize well to unseen data and performs poorly on the test data. . 6. Adjust the model parameters . Due to its performance on the training data, it&#39;s clear our model is learning something. However, performing well on the training data is like going well in the classroom but failing to use your skills in real life. . Ideally, we&#39;d like our model to perform as well on the test data as it does on the training data. . So our next steps will be to try and prevent our model overfitting. A couple of ways to prevent overfitting include: . Get more data - Having more data gives the model more opportunities to learn patterns, patterns which may be more generalizable to new examples. | Simplify model - If the current model is already overfitting the training data, it may be too complicated of a model. This means it&#39;s learning the patterns of the data too well and isn&#39;t able to generalize well to unseen data. One way to simplify a model is to reduce the number of layers it uses or to reduce the number of hidden units in each layer. | Use data augmentation - Data augmentation manipulates the training data in a way so that&#39;s harder for the model to learn as it artificially adds more variety to the data. If a model is able to learn patterns in augmented data, the model may be able to generalize better to unseen data. | Use transfer learning - Transfer learning involves leverages the patterns (also called pretrained weights) one model has learned to use as the foundation for your own task. In our case, we could use one computer vision model pretrained on a large variety of images and then tweak it slightly to be more specialized for food images.🔑 Note: Preventing overfitting is also referred to as regularization. If you&#39;ve already got an existing dataset, you&#39;re probably most likely to try one or a combination of the last three above options first. . | . Since collecting more data would involve us manually taking more images of food, let&#39;s try the ones we can do from right within the notebook. . How about we simplify our model first? . To do so, we&#39;ll remove two of the convolutional layers, taking the total number of convolutional layers from four to two. . model_10 = Sequential([ Conv2D(10, 3, activation=&#39;relu&#39;, input_shape=(224, 224, 3)), MaxPool2D(), Conv2D(10, 3, activation=&#39;relu&#39;), MaxPool2D(), Flatten(), Dense(10, activation=&#39;softmax&#39;) ]) model_10.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=tf.keras.optimizers.Adam(), metrics=[&#39;accuracy&#39;]) history_10 = model_10.fit(train_data, epochs=5, steps_per_epoch=len(train_data), validation_data=test_data, validation_steps=len(test_data)) . Epoch 1/5 235/235 [==============================] - 42s 178ms/step - loss: 2.6327 - accuracy: 0.1715 - val_loss: 1.9986 - val_accuracy: 0.2832 Epoch 2/5 235/235 [==============================] - 41s 176ms/step - loss: 1.9330 - accuracy: 0.3254 - val_loss: 1.9260 - val_accuracy: 0.3116 Epoch 3/5 235/235 [==============================] - 41s 177ms/step - loss: 1.6897 - accuracy: 0.4278 - val_loss: 1.9498 - val_accuracy: 0.3240 Epoch 4/5 235/235 [==============================] - 41s 176ms/step - loss: 1.3091 - accuracy: 0.5829 - val_loss: 2.0369 - val_accuracy: 0.3264 Epoch 5/5 235/235 [==============================] - 41s 176ms/step - loss: 0.8338 - accuracy: 0.7584 - val_loss: 2.3501 - val_accuracy: 0.2880 . plot_loss_curves(history_10) . Hmm... even with a simplifed model, it looks like our model is still dramatically overfitting the training data. . What else could we try? . How about data augmentation? . Data augmentation makes it harder for the model to learn on the training data and in turn, hopefully making the patterns it learns more generalizable to unseen data. . To create augmented data, we&#39;ll recreate a new ImageDataGenerator instance, this time adding some parameters such as rotation_range and horizontal_flip to manipulate our images. . train_datagen_augmented = ImageDataGenerator(rescale=1/255., rotation_range=0.2, width_shift_range=0.2, height_shift_range=0.2, zoom_range=0.2, horizontal_flip=True) train_data_augmented = train_datagen_augmented.flow_from_directory(train_dir, target_size=(224, 224), batch_size=32, class_mode=&#39;categorical&#39;) . Found 7500 images belonging to 10 classes. . Now we&#39;ve got augmented data, let&#39;s see how it works with the same model as before (model_10). . Rather than rewrite the model from scratch, we can clone it using a handy function in TensorFlow called clone_model which can take an existing model and rebuild it in the same format. . The cloned version will not include any of the weights (patterns) the original model has learned. So when we train it, it&#39;ll be like training a model from scratch. . 🔑 Note: One of the key practices in deep learning and machine learning in general is to be a serial experimenter. That&#39;s what we&#39;re doing here. Trying something, seeing if it works, then trying something else. A good experiment setup also keeps track of the things you change, for example, that&#39;s why we&#39;re using the same model as before but with different data. The model stays the same but the data changes, this will let us know if augmented training data has any influence over performance. . model_11 = tf.keras.models.clone_model(model_10) # Compile the cloned model (same setup as used for model_10) model_11.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=tf.keras.optimizers.Adam(), metrics=[&quot;accuracy&quot;]) # Fit the model history_11 = model_11.fit(train_data_augmented, # use augmented data epochs=5, steps_per_epoch=len(train_data_augmented), validation_data=test_data, validation_steps=len(test_data)) . Epoch 1/5 235/235 [==============================] - 105s 445ms/step - loss: 2.3695 - accuracy: 0.1412 - val_loss: 2.0692 - val_accuracy: 0.2684 Epoch 2/5 235/235 [==============================] - 104s 443ms/step - loss: 2.1072 - accuracy: 0.2545 - val_loss: 2.0075 - val_accuracy: 0.2880 Epoch 3/5 235/235 [==============================] - 104s 443ms/step - loss: 2.0748 - accuracy: 0.2601 - val_loss: 1.9161 - val_accuracy: 0.3356 Epoch 4/5 235/235 [==============================] - 104s 444ms/step - loss: 2.0236 - accuracy: 0.2716 - val_loss: 1.9280 - val_accuracy: 0.3332 Epoch 5/5 235/235 [==============================] - 104s 444ms/step - loss: 1.9839 - accuracy: 0.3207 - val_loss: 1.8343 - val_accuracy: 0.3616 . You can see it each epoch takes longer than the previous model. This is because our data is being augmented on the fly on the CPU as it gets loaded onto the GPU, in turn, increasing the amount of time between each epoch. . How do our model&#39;s training curves look? . plot_loss_curves(history_11) . Woah! That&#39;s looking much better, the loss curves are much closer to eachother. Although our model didn&#39;t perform as well on the augmented training set, it performed much better on the validation dataset. . It even looks like if we kept it training for longer (more epochs) the evaluation metrics might continue to improve. . 7. Repeat until satisfied . We could keep going here. Restructuring our model&#39;s architecture, adding more layers, trying it out, adjusting the learning rate, trying it out, trying different methods of data augmentation, training for longer. But as you could image, this could take a fairly long time. . Good thing there&#39;s still one trick we haven&#39;t tried yet and that&#39;s transfer learning. . However, we&#39;ll save that for the next notebook where you&#39;ll see how rather than design our own models from scratch we leverage the patterns another model has learned for our own task. . In the meantime, let&#39;s make a prediction with our trained multi-class model. . Making a prediction with our trained model . What good is a model if you can&#39;t make predictions with it? . Let&#39;s first remind ourselves of the classes our multi-class model has been trained on and then we&#39;ll download some of own custom images to work with. . class_names . array([&#39;chicken_curry&#39;, &#39;chicken_wings&#39;, &#39;fried_rice&#39;, &#39;grilled_salmon&#39;, &#39;hamburger&#39;, &#39;ice_cream&#39;, &#39;pizza&#39;, &#39;ramen&#39;, &#39;steak&#39;, &#39;sushi&#39;], dtype=&#39;&lt;U14&#39;) . Beautiful, now let&#39;s get some of our custom images. . If you&#39;re using Google Colab, you could also upload some of your own images via the files tab. . !wget -q https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-pizza-dad.jpeg !wget -q https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-steak.jpeg !wget -q https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-hamburger.jpeg !wget -q https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-sushi.jpeg . Okay, we&#39;ve got some custom images to try, let&#39;s use the pred_and_plot function to make a prediction with model_11 on one of the images and plot it. . pred_and_plot(model=model_11, filename=&quot;03-steak.jpeg&quot;, class_names=class_names) . Hmm... it looks like our model got the prediction wrong, how about we try another? . pred_and_plot(model_11, &quot;03-sushi.jpeg&quot;, class_names) . And again, it&#39;s predicting chicken_curry for some reason. . How about one more? . pred_and_plot(model_11, &quot;03-pizza-dad.jpeg&quot;, class_names) . chicken_curry again? There must be something wrong... . I think it might have to do with our pred_and_plot function. . Let&#39;s makes a prediction without using the function and see where it might be going wrong. . img = load_and_prep_image(&quot;03-steak.jpeg&quot;) # Make a prediction pred = model_11.predict(tf.expand_dims(img, axis=0)) # Match the prediction class to the highest prediction probability pred_class = class_names[pred.argmax()] plt.imshow(img) plt.title(pred_class) plt.axis(False); . Much better! There must be something up with our pred_and_plot function. . And I think I know what it is. . The pred_and_plot function was designed to be used with binary classification models where as our current model is a multi-class classification model. . The main difference lies in the output of the predict function. . pred = model_11.predict(tf.expand_dims(img, axis=0)) pred . array([[0.04194485, 0.08470038, 0.0369736 , 0.13461705, 0.11786617, 0.15232824, 0.02880317, 0.08542344, 0.25056237, 0.06678072]], dtype=float32) . Since our model has a &#39;softmax&#39; activation function and 10 output neurons, it outputs a prediction probability for each of the classes in our model. . The class with the highest probability is what the model believes the image contains. . We can find the maximum value index using argmax and then use that to index our class_names list to output the predicted class. . class_names[pred.argmax()] . &#39;steak&#39; . Knowing this, we can readjust our pred_and_plot function to work with multiple classes as well as binary classes. . def pred_and_plot(model, filename, class_names): &quot;&quot;&quot; Imports an image located at filename, makes a prediction on it with a trained model and plots the image with the predicted class as the title. &quot;&quot;&quot; # Import the target image and preprocess it img = load_and_prep_image(filename) # Make a prediction pred = model.predict(tf.expand_dims(img, axis=0)) # Get the predicted class if len(pred[0]) &gt; 1: # check for multi-class pred_class = class_names[pred.argmax()] # if more than one output, take the max else: pred_class = class_names[int(tf.round(pred)[0][0])] # if only one output, round # Plot the image and predicted class plt.imshow(img) plt.title(f&quot;Prediction: {pred_class}&quot;) plt.axis(False); . Let&#39;s try it out. If we&#39;ve done it right, using different images should lead to different outputs (rather than chicken_curry every time). . pred_and_plot(model_11, &quot;03-steak.jpeg&quot;, class_names) . pred_and_plot(model_11, &quot;03-sushi.jpeg&quot;, class_names) . pred_and_plot(model_11, &quot;03-pizza-dad.jpeg&quot;, class_names) . pred_and_plot(model_11, &quot;03-hamburger.jpeg&quot;, class_names) . Our model&#39;s predictions aren&#39;t very good, this is because it&#39;s only performing at ~35% accuracy on the test dataset. . Saving and loading our model . Once you&#39;ve trained a model, you probably want to be able to save it and load it somewhere else. . To do so, we can use the save and load_model functions. . model_11.save(&quot;saved_trained_model&quot;) . INFO:tensorflow:Assets written to: saved_trained_model/assets . loaded_model_11 = tf.keras.models.load_model(&quot;saved_trained_model&quot;) loaded_model_11.evaluate(test_data) . 79/79 [==============================] - 10s 128ms/step - loss: 1.8343 - accuracy: 0.3616 . [1.8343267440795898, 0.36160001158714294] . model_11.evaluate(test_data) . 79/79 [==============================] - 10s 128ms/step - loss: 1.8343 - accuracy: 0.3616 . [1.8343265056610107, 0.36160001158714294] . &#128736; Exercises . Spend 20-minutes reading and interacting with the CNN explainer website. . What are the key terms? e.g. explain convolution in your own words, pooling in your own words | . | Play around with the &quot;understanding hyperparameters&quot; section in the CNN explainer website for 10-minutes. . What is the kernel size? | What is the stride? | How could you adjust each of these in TensorFlow code? | . | Take 10 photos of two different things and build your own CNN image classifier using the techniques we&#39;ve built here. . | Find an ideal learning rate for a simple convolutional neural network model on your the 10 class dataset. . | &#128214; Extra-curriculum . Watch: MIT&#39;s Introduction to Deep Computer Vision lecture. This will give you a great intuition behind convolutional neural networks. . | Watch: Deep dive on mini-batch gradient descent by deeplearning.ai. If you&#39;re still curious about why we use batches to train models, this technical overview covers many of the reasons why. . | Read: CS231n Convolutional Neural Networks for Visual Recognition class notes. This will give a very deep understanding of what&#39;s going on behind the scenes of the convolutional neural network architectures we&#39;re writing. . | Read: &quot;A guide to convolution arithmetic for deep learning&quot;. This paper goes through all of the mathematics running behind the scenes of our convolutional layers. . | Code practice: TensorFlow Data Augmentation Tutorial. For a more in-depth introduction on data augmentation with TensorFlow, spend an hour or two reading through this tutorial. . |",
            "url": "https://ashikshafi08.github.io/fastpages/daniels_course/2021/04/11/cnn-tensorflow.html",
            "relUrl": "/daniels_course/2021/04/11/cnn-tensorflow.html",
            "date": " • Apr 11, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ashikshafi08.github.io/fastpages/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ashikshafi08.github.io/fastpages/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hey it’s Ashik here! I ma Machine Learning Practitioner. .",
          "url": "https://ashikshafi08.github.io/fastpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ashikshafi08.github.io/fastpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}